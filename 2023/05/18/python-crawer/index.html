<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>使用 Python 做数据采集(爬虫) | Calico's Space</title><meta name="author" content="Calico"><meta name="copyright" content="Calico"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="python数据采集爬虫（crawler）也经常被称为网络蜘蛛（spider），是按照一定的规则自动浏览网站并获取所需信息的机器人程序（自动化脚本代码），被广泛的应用于互联网搜索引擎和数据采集。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接，网络爬虫正是通过网页中的超链接信息，不断获得网络上其它页面的地址，然后持续的进行数据采集。正因如此，网络数据采集的过程就">
<meta property="og:type" content="article">
<meta property="og:title" content="使用 Python 做数据采集(爬虫)">
<meta property="og:url" content="http://xeons.cn/2023/05/18/python-crawer/index.html">
<meta property="og:site_name" content="Calico&#39;s Space">
<meta property="og:description" content="python数据采集爬虫（crawler）也经常被称为网络蜘蛛（spider），是按照一定的规则自动浏览网站并获取所需信息的机器人程序（自动化脚本代码），被广泛的应用于互联网搜索引擎和数据采集。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接，网络爬虫正是通过网页中的超链接信息，不断获得网络上其它页面的地址，然后持续的进行数据采集。正因如此，网络数据采集的过程就">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xeons.cn/images/calico.png">
<meta property="article:published_time" content="2023-05-18T05:47:07.000Z">
<meta property="article:author" content="Calico">
<meta property="article:tag" content="python">
<meta property="article:tag" content="数据采集">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://xeons.cn/images/calico.png"><link rel="shortcut icon" href="/images/calico-ss.png"><link rel="canonical" href="http://xeons.cn/2023/05/18/python-crawer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><meta/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6f97e3791752fae830e3db5ba194c6cb";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '使用 Python 做数据采集(爬虫)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-03-12 21:47:48'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="Calico's Space" type="application/atom+xml">
</head><body><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="/css/loading_bar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/calico.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list hide"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/demopage/"><i class="fa-fw fas fa-music"></i><span> Theme测试页</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/def_top_image.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Calico's Space"><span class="site-name">Calico's Space</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list hide"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/demopage/"><i class="fa-fw fas fa-music"></i><span> Theme测试页</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">使用 Python 做数据采集(爬虫)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2023-05-18T05:47:07.000Z" title="发表于 2023-05-18 13:47:07">2023-05-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/python/">python</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/">数据采集</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="使用 Python 做数据采集(爬虫)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="python数据采集"><a href="#python数据采集" class="headerlink" title="python数据采集"></a>python数据采集</h2><p>爬虫（crawler）也经常被称为网络蜘蛛（spider），是按照一定的规则自动浏览网站并获取所需信息的机器人程序（自动化脚本代码），被广泛的应用于互联网搜索引擎和数据采集。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接，网络爬虫正是通过网页中的超链接信息，不断获得网络上其它页面的地址，然后持续的进行数据采集。正因如此，网络数据采集的过程就像一个爬虫或者蜘蛛在网络上漫游，所以才被形象的称为爬虫或者网络蜘蛛。</p>
<p>主要的应用领域:</p>
<ol>
<li>搜索引擎</li>
<li>新闻聚合</li>
<li>社交应用</li>
<li>舆情监控</li>
<li>行业数据</li>
</ol>
<h3 id="主要的实用库，以及前期的页面调研"><a href="#主要的实用库，以及前期的页面调研" class="headerlink" title="主要的实用库，以及前期的页面调研"></a>主要的实用库，以及前期的页面调研</h3><pre><code>对于数据页面或者数据接口的研究是第一步，你需要掌握html结构，页面渲染方式（ajax、iframe或者其他懒加载机制)，以及安全验证（登录)，session 保持、auth 等。 
</code></pre>
<ol>
<li><p>httpie, 一个命令行的页面请求工具，可以用于前期的page页面结构观察。<br><a target="_blank" rel="noopener" href="https://httpie.io/docs/cli/universal">doc文档</a></p>
</li>
<li><p>buildwith 获取网站使用技术</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install buildwith </span><br></pre></td></tr></table></figure>
</li>
<li><p>requests 库<br><a target="_blank" rel="noopener" href="https://docs.python-requests.org/en/latest/">doc文档</a><br>这是一个HTTP库，可以用于发送HTTP请求，获取响应，解析响应，获取数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line">r = requests.post(<span class="string">&#x27;https://httpbin.org/post&#x27;</span>, data=&#123;<span class="string">&#x27;key&#x27;</span>: <span class="string">&#x27;value&#x27;</span>&#125;)</span><br><span class="line">r = requests.put(<span class="string">&#x27;https://httpbin.org/put&#x27;</span>, data=&#123;<span class="string">&#x27;key&#x27;</span>: <span class="string">&#x27;value&#x27;</span>&#125;)</span><br><span class="line">r = requests.delete(<span class="string">&#x27;https://httpbin.org/delete&#x27;</span>)</span><br><span class="line">r = requests.head(<span class="string">&#x27;https://httpbin.org/get&#x27;</span>)</span><br><span class="line">r = requests.options(<span class="string">&#x27;https://httpbin.org/get&#x27;</span>)</span><br><span class="line">payload = &#123;<span class="string">&#x27;key1&#x27;</span>: <span class="string">&#x27;value1&#x27;</span>, <span class="string">&#x27;key2&#x27;</span>: <span class="string">&#x27;value2&#x27;</span>&#125;</span><br><span class="line">r = requests.get(<span class="string">&#x27;https://httpbin.org/get&#x27;</span>, params=payload)</span><br><span class="line"></span><br><span class="line">r.text() <span class="comment">#  获取响应文本</span></span><br><span class="line">r.json() <span class="comment">#  获取响应json</span></span><br><span class="line">r.raw() <span class="comment">#  获取响应原始数据</span></span><br><span class="line">r.status_code() <span class="comment">#  获取响应状态码</span></span><br><span class="line">r.headers() <span class="comment">#  获取响应头</span></span><br><span class="line">r.cookies() <span class="comment">#  获取响应cookies</span></span><br><span class="line">r.history() <span class="comment">#  获取响应历史记录</span></span><br><span class="line">r.elapsed() <span class="comment">#  获取响应时间</span></span><br><span class="line">r.encoding() <span class="comment">#  获取响应编码</span></span><br><span class="line">r.content() <span class="comment">#  获取响应内容</span></span><br><span class="line">r.request() <span class="comment">#  获取请求对象</span></span><br><span class="line">r.url() <span class="comment">#  获取响应url</span></span><br><span class="line">r.raise_for_status() <span class="comment">#  获取响应状态码, 200情况下 是None</span></span><br><span class="line">r.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取简单流与原始流 </span></span><br><span class="line">r = requests.get(<span class="string">&#x27;https://api.github.com/events&#x27;</span>, stream=<span class="literal">True</span>)</span><br><span class="line">r.raw</span><br><span class="line">r.raw.read(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取响应流 带buff，需要注意的与 raw不同，1 需要关闭流，2 默认已经decode gzip 和默认的压缩格式</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fd:</span><br><span class="line">    <span class="keyword">for</span> chunk <span class="keyword">in</span> r.iter_content(chunk_size=<span class="number">128</span>):</span><br><span class="line">        fd.write(chunk)</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件上传</span></span><br><span class="line">files = &#123;<span class="string">&#x27;file&#x27;</span>: <span class="built_in">open</span>(<span class="string">&#x27;report.xls&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">&#x27;https://httpbin.org/post&#x27;</span>, files=files)</span><br><span class="line">r.text()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="爬取数据的一个流程"><a href="#爬取数据的一个流程" class="headerlink" title="爬取数据的一个流程"></a>爬取数据的一个流程</h3><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/05/18/python-crawer/crawler-workflow.png" class="">


<h3 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h3><h4 id="1-一个最原生的requests-re-爬取网页的例子"><a href="#1-一个最原生的requests-re-爬取网页的例子" class="headerlink" title="1. 一个最原生的requests + re 爬取网页的例子"></a>1. 一个最原生的requests + re 爬取网页的例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> requests.adapters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过正则表达式获取class属性为title且标签体不以&amp;开头的span标签并用捕获组提取标签内容</span></span><br><span class="line">pattern1 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;span class=&quot;title&quot;&gt;([^&amp;]*?)&lt;/span&gt;&#x27;</span>)</span><br><span class="line"><span class="comment"># 通过正则表达式获取class属性为rating_num的span标签并用捕获组提取标签内容</span></span><br><span class="line">pattern2 = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;span class=&quot;rating_num&quot;.*?&gt;(.*?)&lt;/span&gt;&#x27;</span>)</span><br><span class="line"><span class="comment"># 获取豆瓣电影top250</span></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    resp = requests.get(</span><br><span class="line">        url =<span class="string">f&#x27;https://movie.douban.com/top250?start=<span class="subst">&#123;(p - <span class="number">1</span>) * <span class="number">25</span>&#125;</span>&#x27;</span>,</span><br><span class="line">        <span class="comment"># 设置头，有些网站必须设置头才可以访问. 这里页可模拟baiduspider的请求头</span></span><br><span class="line">        headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;</span>&#125;</span><br><span class="line">    )</span><br><span class="line">    titles = pattern1.findall(resp.text)</span><br><span class="line">    ranks = pattern2.findall(resp.text)</span><br><span class="line">    <span class="comment"># 使用zip压缩两个列表，循环遍历所有的电影标题和评分</span></span><br><span class="line">    <span class="keyword">for</span> title, rank <span class="keyword">in</span> <span class="built_in">zip</span>(titles, ranks):</span><br><span class="line">        <span class="built_in">print</span>(title, rank)</span><br><span class="line">    <span class="comment"># 随机休眠1-5秒，避免爬取页面过于频繁</span></span><br><span class="line">    time.sleep(random.random() * <span class="number">4</span> + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<ol start="2">
<li><h4 id="使用代理"><a href="#使用代理" class="headerlink" title="使用代理"></a>使用代理</h4><p> 一般网站基于自身安全和带宽容量的浪费，在网关或者入口处都会有防爬和流量控制，所以需要使用代理来绕过网关，防止被封禁。</p>
<p> 这里建议使用商业代理（免费往往不太稳定），比如:蘑菇代理、芝麻代理、快代理等<br> 这里需要去他们的官网注册来得到代理ip。一般上面的代理服务商都会提供完善的demo给我们。</p>
</li>
<li><h4 id="其他常用的html解析库"><a href="#其他常用的html解析库" class="headerlink" title="其他常用的html解析库"></a>其他常用的html解析库</h4><blockquote>
<p> 不过这里个人提倡原生方式，类似xpath或者bef这类其实是将text生成一个内存的dom树，然后通过xpath语法提取数据，这将消耗较多内存</p>
</blockquote>
<table>
<thead>
<tr>
<th>解析方式</th>
<th>对应的模块</th>
<th>速度</th>
<th>使用难度</th>
</tr>
</thead>
<tbody><tr>
<td>正则表达式解析</td>
<td><code>re</code></td>
<td>快</td>
<td>困难</td>
</tr>
<tr>
<td>XPath 解析</td>
<td><code>lxml</code></td>
<td>快</td>
<td>一般</td>
</tr>
<tr>
<td>CSS 选择器解析</td>
<td><code>bs4</code>或<code>pyquery</code></td>
<td>不确定</td>
<td>简单</td>
</tr>
</tbody></table>
<ul>
<li><p>BeautifulSoup</p>
<p>关于 BeautifulSoup ，可以参考它的<a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/">官方文档</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  <span class="comment"># 创建BeautifulSoup对象</span></span><br><span class="line">soup = bs4.BeautifulSoup(resp.text, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="comment"># 通过CSS选择器从页面中提取包含电影标题的span标签</span></span><br><span class="line">title_spans = soup.select(<span class="string">&#x27;div.info &gt; div.hd &gt; a &gt; span:nth-child(1)&#x27;</span>)</span><br><span class="line"><span class="comment"># 通过CSS选择器从页面中提取包含电影评分的span标签</span></span><br><span class="line">rank_spans = soup.select(<span class="string">&#x27;div.info &gt; div.bd &gt; div &gt; span.rating_num&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> title_span, rank_span <span class="keyword">in</span> <span class="built_in">zip</span>(title_spans, rank_spans):</span><br><span class="line">    <span class="built_in">print</span>(title_span.text, rank_span.text)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


</li>
<li><p>lxml  一个基于xpath的解析dom库，对于xpath 需要了解相关知识.</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tree = etree.HTML(resp.text)</span><br><span class="line"><span class="comment"># 通过XPath语法从页面中提取电影标题</span></span><br><span class="line">title_spans = tree.xpath(<span class="string">&#x27;//*[@id=&quot;content&quot;]/div/div[1]/ol/li/div/div[2]/div[1]/a/span[1]&#x27;</span>)</span><br><span class="line"><span class="comment"># 通过XPath语法从页面中提取电影评分</span></span><br><span class="line">rank_spans = tree.xpath(<span class="string">&#x27;//*[@id=&quot;content&quot;]/div/div[1]/ol/li[1]/div/div[2]/div[2]/div/span[2]&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> title_span, rank_span <span class="keyword">in</span> <span class="built_in">zip</span>(title_spans, rank_spans):</span><br><span class="line">    <span class="built_in">print</span>(title_span.text, rank_span.text)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><h4 id="并发在抓取中的使用"><a href="#并发在抓取中的使用" class="headerlink" title="并发在抓取中的使用"></a>并发在抓取中的使用</h4><p>数据采集（爬虫）在python并发情形中是最符合 大量io操作的，所以使用线程和异步IO是在爬虫中比较常用的。尤其是深度采集的需求时.</p>
<ul>
<li>线程池<blockquote>
<p>主要片段:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> os</span><br><span class="line">  <span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> ThreadPoolExecutor(max_workers=<span class="number">16</span>) <span class="keyword">as</span> pool:</span><br><span class="line">      <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">          resp = requests.get(<span class="string">f&#x27;https://image.so.com/zjl?ch=beauty&amp;sn=<span class="subst">&#123;page * <span class="number">30</span>&#125;</span>&#x27;</span>)</span><br><span class="line">          <span class="keyword">if</span> resp.status_code == <span class="number">200</span>:</span><br><span class="line">              pic_dict_list = resp.json()[<span class="string">&#x27;list&#x27;</span>]</span><br><span class="line">              <span class="keyword">for</span> pic_dict <span class="keyword">in</span> pic_dict_list:</span><br><span class="line">                  pool.submit(download_picture, pic_dict[<span class="string">&#x27;qhimg_url&#x27;</span>])</span><br><span class="line">  ```    </span><br><span class="line"></span><br><span class="line">* 异步IO</span><br><span class="line">  </span><br><span class="line">  我们使用`aiohttp`将上面的代码修改为异步 I/O 的版本。为了以异步 I/O 的方式实现网络资源的获取和写文件操作，我们首先得安装三方库`aiohttp`和`aiofile`，命令如下所示。</span><br><span class="line">  ``` shell</span><br><span class="line">  pip install aiohttp aiofile  </span><br></pre></td></tr></table></figure>
下面是一个完整的demo：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> aiofile</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">download_picture</span>(<span class="params">session, url</span>):</span><br><span class="line">    filename = url[url.rfind(<span class="string">&#x27;/&#x27;</span>) + <span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url, ssl=<span class="literal">False</span>) <span class="keyword">as</span> resp:</span><br><span class="line">        <span class="keyword">if</span> resp.status == <span class="number">200</span>:</span><br><span class="line">            data = <span class="keyword">await</span> resp.read()</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">with</span> aiofile.async_open(<span class="string">f&#x27;images/beauty/<span class="subst">&#123;filename&#125;</span>&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">                <span class="keyword">await</span> file.write(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">fetch_json</span>():</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">with</span> session.get(</span><br><span class="line">                url=<span class="string">f&#x27;https://image.so.com/zjl?ch=beauty&amp;sn=<span class="subst">&#123;page * <span class="number">30</span>&#125;</span>&#x27;</span>,</span><br><span class="line">                ssl=<span class="literal">False</span></span><br><span class="line">            ) <span class="keyword">as</span> resp:</span><br><span class="line">                <span class="keyword">if</span> resp.status == <span class="number">200</span>:</span><br><span class="line">                    json_str = <span class="keyword">await</span> resp.text()</span><br><span class="line">                    result = json.loads(json_str)</span><br><span class="line">                    <span class="keyword">for</span> pic_dict <span class="keyword">in</span> result[<span class="string">&#x27;list&#x27;</span>]:</span><br><span class="line">                        <span class="keyword">await</span> download_picture(session, pic_dict[<span class="string">&#x27;qhimg_url&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;images/beauty&#x27;</span>):</span><br><span class="line">        os.makedirs(<span class="string">&#x27;images/beauty&#x27;</span>)</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    loop.run_until_complete(fetch_json())</span><br><span class="line">    loop.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><h4 id="Selenium-with-Python"><a href="#Selenium-with-Python" class="headerlink" title="Selenium with Python"></a>Selenium with Python</h4><p> selenium 是一个用于自动化测试的开源工具，可以驱动浏览器，模拟用户操作，实现自动化测试。 随着css和js技术发展，很多站点数据都是由前端的js渲染或者异步加载的，这个时候如果需要提取数据即可通过selenium来完成。 通过selenium可以模拟js执行，按钮点击等大多数浏览器事件。 更多详细内容可以参考<a target="_blank" rel="noopener" href="https://www.selenium.dev/documentation/zh-cn/">官方文档</a>。<br> <a target="_blank" rel="noopener" href="https://selenium-python-zh.readthedocs.io/en/latest/installation.html#id2">python-selenium文档</a></p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"></span><br><span class="line">DOWNLOAD_PATH = <span class="string">&#x27;images/&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_picture</span>(<span class="params">picture_url: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    下载保存图片</span></span><br><span class="line"><span class="string">    :param picture_url: 图片的URL</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    filename = picture_url[picture_url.rfind(<span class="string">&#x27;/&#x27;</span>) + <span class="number">1</span>:]</span><br><span class="line">    resp = requests.get(picture_url)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(DOWNLOAD_PATH, filename), <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        file.write(resp.content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(DOWNLOAD_PATH):</span><br><span class="line">    os.makedirs(DOWNLOAD_PATH)</span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">&#x27;https://image.so.com/z?ch=beauty&#x27;</span>)</span><br><span class="line">browser.implicitly_wait(<span class="number">10</span>)</span><br><span class="line">kw_input = browser.find_element(By.CSS_SELECTOR, <span class="string">&#x27;input[name=q]&#x27;</span>)</span><br><span class="line">kw_input.send_keys(<span class="string">&#x27;苍老师&#x27;</span>)</span><br><span class="line">kw_input.send_keys(Keys.ENTER)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    browser.execute_script(</span><br><span class="line">        <span class="string">&#x27;document.documentElement.scrollTop = document.documentElement.scrollHeight&#x27;</span></span><br><span class="line">    )</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">imgs = browser.find_elements(By.CSS_SELECTOR, <span class="string">&#x27;div.waterfall img&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> ThreadPoolExecutor(max_workers=<span class="number">32</span>) <span class="keyword">as</span> pool:</span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> imgs:</span><br><span class="line">        pic_url = img.get_attribute(<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">        pool.submit(download_picture, pic_url)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>


</li>
<li><h4 id="爬虫框架Scrapy"><a href="#爬虫框架Scrapy" class="headerlink" title="爬虫框架Scrapy"></a>爬虫框架Scrapy</h4><p>scrapy是一个基于python的爬虫框架，可以快速实现爬虫，并且可以方便的进行数据存储。scrapy的安装可以参考<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/intro/install.html">官方文档</a>。</p>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/w3cnote/scrapy-detail.html">doc2</a></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/05/18/python-crawer/scrapy.png" class=""></li>
</ol>
<p>  下面是一个简单的scrapydemo，未涉及部分可查看官方doc.</p>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建项目</span></span><br><span class="line">scrapy startproject scrapydemo</span><br><span class="line"><span class="comment"># 创建爬虫主入口</span></span><br><span class="line">scrapy genspider doubanbook book.douban.com </span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建采集数据的对象orm </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubanBookItem</span>(scrapy.Item):</span><br><span class="line">  <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">  <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">  title = scrapy.Field()</span><br><span class="line">  score = scrapy.Field()</span><br><span class="line">  motto = scrapy.Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 采集逻辑编写</span></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Iterable</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapydemo.items <span class="keyword">import</span> DoubanBookItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubanbookSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;doubanbook&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;book.douban.com&quot;</span>]</span><br><span class="line">    <span class="comment"># 固定urls</span></span><br><span class="line">    <span class="comment">#start_urls = [&quot;https://book.douban.com/top250?start=0&quot;]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=<span class="string">f&quot;https://book.douban.com/top250?start=<span class="subst">&#123;page*<span class="number">25</span>&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># print(response)  # &lt;200 http://www.4399.com/flash/&gt;</span></span><br><span class="line">        <span class="comment"># print(response.text)  # 打印页面源代码</span></span><br><span class="line">        <span class="comment"># response.xpath()  # 通过xpath解析数据</span></span><br><span class="line">        <span class="comment"># response.css()  # 通过css解析数据</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取名称</span></span><br><span class="line">        <span class="comment"># txt = response.xpath(&#x27;//ul[@class=&quot;n-game cf&quot;]/li/a/b/text()&#x27;)</span></span><br><span class="line">        <span class="comment"># txt 列表中的每一项是一个Selector：</span></span><br><span class="line">        <span class="comment"># &lt;Selector query=&#x27;//ul[@class=&quot;n-game cf&quot;]/li/a/b/text()&#x27; data=&#x27;逃离克莱蒙特城堡&#x27;&gt;]</span></span><br><span class="line">        <span class="comment"># 要通过extract()方法拿到data中的内容</span></span><br><span class="line">        <span class="comment"># print(txt)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># txt = response.xpath(&#x27;//ul[@class=&quot;n-game cf&quot;]/li/a/b/text()&#x27;).extract()</span></span><br><span class="line">        <span class="comment"># print(txt)  # </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 也可以先拿到每个li，然后再提取名字</span></span><br><span class="line">        <span class="comment"># lis = response.xpath(&#x27;//ul[@class=&quot;n-game cf&quot;]/li&#x27;)</span></span><br><span class="line">        book_items = response.css(<span class="string">&#x27;#content &gt; div &gt; div.article &gt; div &gt; table&#x27;</span>)</span><br><span class="line">        <span class="comment">#print(len(book_items))</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> book_items:</span><br><span class="line">            item = DoubanBookItem()</span><br><span class="line">            item[<span class="string">&#x27;title&#x27;</span>] = i.css(<span class="string">&#x27; tr &gt; td:nth-child(2) &gt; div.pl2 &gt; a::text&#x27;</span>).extract_first().strip()</span><br><span class="line">            item[<span class="string">&#x27;score&#x27;</span>] = i.css(<span class="string">&#x27; tr &gt; td:nth-child(2) &gt; div.star.clearfix &gt; span.rating_nums::text&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;img&#x27;</span>] = i.css(<span class="string">&#x27; tr &gt; td:nth-child(1) &gt; a &gt; img::attr(&quot;src&quot;)&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;motto&#x27;</span>] = i.css(<span class="string">&#x27; tr &gt; td:nth-child(2) &gt; p.quote &gt; span::text&#x27;</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 页面下的分页url全部提取出来</span></span><br><span class="line">        <span class="comment">#hrefs = response.css(&#x27;#content &gt; div &gt; div.article &gt; div.paginator &gt; a::attr(&quot;href&quot;)&#x27;)</span></span><br><span class="line">        <span class="comment">#for href in hrefs:</span></span><br><span class="line">        <span class="comment">#    full_url = response.urljoin(href.extract())</span></span><br><span class="line">        <span class="comment">#    yield scrapy.Request(url=full_url)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 存储数据</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScrapydemoPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 存储到mysql的pipline</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Doubanbook2DbPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;爬虫开始运行&#x27;</span>)</span><br><span class="line">        self.conn = pymysql.connect(host=<span class="string">&#x27;localhost&#x27;</span>, port=<span class="number">3306</span>, user=<span class="string">&#x27;root&#x27;</span>, passwd=<span class="string">&#x27;123456&#x27;</span>, db=<span class="string">&#x27;doubanbook&#x27;</span>, charset=<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.conn = <span class="literal">None</span></span><br><span class="line">        self.cursor = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        sql = <span class="string">&#x27;insert into book(title, score, motto, img) values(%s, %s, %s, %s)&#x27;</span></span><br><span class="line">        self.cursor.execute(sql, (item[<span class="string">&#x27;title&#x27;</span>], item[<span class="string">&#x27;score&#x27;</span>], item[<span class="string">&#x27;motto&#x27;</span>], item[<span class="string">&#x27;img&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Doubanbook2Pipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.f = <span class="built_in">open</span>(<span class="string">&#x27;doubanbook.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.f.write(item[<span class="string">&#x27;title&#x27;</span>] + <span class="string">&#x27; &#x27;</span> + item[<span class="string">&#x27;score&#x27;</span>] + <span class="string">&#x27; &#x27;</span> + item[<span class="string">&#x27;motto&#x27;</span>] + <span class="string">&#x27; &#x27;</span> + item[<span class="string">&#x27;img&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.f.close() </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>   最后setting.py中需要配置pipeline ，以及对请求的header和浏览器的设置，防止403错误。</p>
<p>   USER_AGENT</p>
<p>   DEFAULT_REQUEST_HEADERS &#x3D; {<br>   “Accept”: “text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,<em>&#x2F;</em>;q&#x3D;0.8”,<br>   “Accept-Language”: “en”,<br>    }</p>
<ol start="7">
<li><h4 id="反爬的一些应对方式"><a href="#反爬的一些应对方式" class="headerlink" title="反爬的一些应对方式"></a>反爬的一些应对方式</h4><ol>
<li><p>构造合理的HTTP请求头。</p>
<ul>
<li><p>Accept</p>
</li>
<li><p>User-Agent</p>
</li>
<li><p>Referer</p>
</li>
<li><p>Accept-Encoding</p>
</li>
<li><p>Accept-Language</p>
</li>
</ul>
</li>
<li><p>检查网站生成的Cookie。</p>
<ul>
<li>有用的插件：<a target="_blank" rel="noopener" href="http://www.editthiscookie.com/">EditThisCookie</a></li>
<li>如何处理脚本动态生成的Cookie</li>
</ul>
</li>
<li><p>抓取动态内容。</p>
<ul>
<li>Selenium + WebDriver</li>
<li>Chrome &#x2F; Firefox - Driver</li>
</ul>
</li>
<li><p>限制爬取的速度。</p>
</li>
<li><p>处理表单中的隐藏域。</p>
<ul>
<li>在读取到隐藏域之前不要提交表单</li>
<li>用RoboBrowser这样的工具辅助提交表单</li>
</ul>
</li>
<li><p>处理表单中的验证码。</p>
<ul>
<li><p>OCR（Tesseract） - 商业项目一般不考虑 </p>
</li>
<li><p>专业识别平台 - 超级鹰 &#x2F; 云打码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChaoClient</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, username, password, soft_id</span>):</span><br><span class="line">        self.username = username</span><br><span class="line">        password =  password.encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        self.password = md5(password).hexdigest()</span><br><span class="line">        self.soft_id = soft_id</span><br><span class="line">        self.base_params = &#123;</span><br><span class="line">            <span class="string">&#x27;user&#x27;</span>: self.username,</span><br><span class="line">            <span class="string">&#x27;pass2&#x27;</span>: self.password,</span><br><span class="line">            <span class="string">&#x27;softid&#x27;</span>: self.soft_id,</span><br><span class="line">        &#125;</span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;Keep-Alive&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)&#x27;</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">post_pic</span>(<span class="params">self, im, codetype</span>):</span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">&#x27;codetype&#x27;</span>: codetype,</span><br><span class="line">        &#125;</span><br><span class="line">        params.update(self.base_params)</span><br><span class="line">        files = &#123;<span class="string">&#x27;userfile&#x27;</span>: (<span class="string">&#x27;captcha.jpg&#x27;</span>, im)&#125;</span><br><span class="line">        r = requests.post(<span class="string">&#x27;http://upload.chaojiying.net/Upload/Processing.php&#x27;</span>, data=params, files=files, headers=self.headers)</span><br><span class="line">        <span class="keyword">return</span> r.json()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    client = ChaoClient(<span class="string">&#x27;用户名&#x27;</span>, <span class="string">&#x27;密码&#x27;</span>, <span class="string">&#x27;软件ID&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;captcha.jpg&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> file:                                                </span><br><span class="line">        <span class="built_in">print</span>(client.post_pic(file, <span class="number">1902</span>))                                          </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>绕开“陷阱”。</p>
<ul>
<li>网页上有诱使爬虫爬取的爬取的隐藏链接（陷阱或蜜罐）</li>
<li>通过Selenium+WebDriver+Chrome判断链接是否可见或在可视区域</li>
</ul>
</li>
<li><p>隐藏身份。</p>
<ul>
<li><p>代理服务 -  快代理 &#x2F; 讯代理 &#x2F; 芝麻代理 &#x2F; 蘑菇代理 &#x2F; 云代理</p>
<p><a target="_blank" rel="noopener" href="https://cuiqingcai.com/5094.html">《爬虫代理哪家强？十大付费代理详细对比评测出炉！》</a></p>
</li>
<li><p>洋葱路由 - 国内需要翻墙才能使用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum -y install tor</span><br><span class="line">useradd admin -d /home/admin</span><br><span class="line">passwd admin</span><br><span class="line">chown -R admin:admin /home/admin</span><br><span class="line">chown -R admin:admin /var/run/tor</span><br><span class="line">tor</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>  python 在数据采集方面，有非常多的工具，如requests，selenium，scrapy等，但是对于一些简单的爬虫，还是使用requests比较方便。<br>  另外，对于一些需要登录的网站，使用selenium可以模拟登录，但是对于一些需要验证码的网站，selenium可能无法解决。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://xeons.cn">Calico</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://xeons.cn/2023/05/18/python-crawer/">http://xeons.cn/2023/05/18/python-crawer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://xeons.cn" target="_blank">Calico's Space</a>！</span></div></div><script>function setClipboardText(event){
    let clipboardData = event.clipboardData || window.clipboardData;
    if (!clipboardData) { return; }
    event.preventDefault();
    let text = window.getSelection().toString();
    if (text) {
        event.preventDefault();
        var copyright = "\n\n---\n著作权归 Calico 所有 \n原文链接: http://xeons.cn/2023/05/18/python-crawer/";
        clipboardData.setData('text/plain', text + copyright);
    }
};
var contents = document.getElementsByClassName("post");
contents[0].addEventListener('copy',function(e){
    setClipboardText(e);
});</script><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/">数据采集</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="/images/calico.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/images/wxpay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/wxpay.png" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/22/python-data-analysis-1p/" title="python 生态下的数据分析学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">python 生态下的数据分析学习</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/05/es-basic/" title="ElasticSearch 介绍"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/05/05/es-basic/logo.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ElasticSearch 介绍</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/06/03/python-base-questions/" title="python 基础题"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/06/03/python-base-questions/python_icon.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-03</div><div class="title">python 基础题</div></div></a></div><div><a href="/2023/05/22/python-data-analysis-1p/" title="python 生态下的数据分析学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-22</div><div class="title">python 生态下的数据分析学习</div></div></a></div><div><a href="/2023/06/03/python-project-deploy/" title="python 项目的部署"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/06/03/python-project-deploy/python_icon.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-03</div><div class="title">python 项目的部署</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81OTczMi8zNjE5NA=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/calico.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Calico</div><div class="author-info__description">It's my blog，Record everything！</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xeonsuo"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xeonsuo" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="https://weibo.com/xeons" target="_blank" title="微博"><i class="fab fa-weibo" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:xeon511@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">python、aiAgent 进化中...</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#python%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="toc-number">1.</span> <span class="toc-text">python数据采集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%9A%84%E5%AE%9E%E7%94%A8%E5%BA%93%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%89%8D%E6%9C%9F%E7%9A%84%E9%A1%B5%E9%9D%A2%E8%B0%83%E7%A0%94"><span class="toc-number">1.1.</span> <span class="toc-text">主要的实用库，以及前期的页面调研</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E4%B8%AA%E6%B5%81%E7%A8%8B"><span class="toc-number">1.2.</span> <span class="toc-text">爬取数据的一个流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="toc-number">1.3.</span> <span class="toc-text">最佳实践</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%B8%80%E4%B8%AA%E6%9C%80%E5%8E%9F%E7%94%9F%E7%9A%84requests-re-%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">1.3.1.</span> <span class="toc-text">1. 一个最原生的requests + re 爬取网页的例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86"><span class="toc-number">1.3.2.</span> <span class="toc-text">使用代理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E5%B8%B8%E7%94%A8%E7%9A%84html%E8%A7%A3%E6%9E%90%E5%BA%93"><span class="toc-number">1.3.3.</span> <span class="toc-text">其他常用的html解析库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%B6%E5%8F%91%E5%9C%A8%E6%8A%93%E5%8F%96%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">1.3.4.</span> <span class="toc-text">并发在抓取中的使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Selenium-with-Python"><span class="toc-number">1.3.5.</span> <span class="toc-text">Selenium with Python</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6Scrapy"><span class="toc-number">1.3.6.</span> <span class="toc-text">爬虫框架Scrapy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8D%E7%88%AC%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BA%94%E5%AF%B9%E6%96%B9%E5%BC%8F"><span class="toc-number">1.3.7.</span> <span class="toc-text">反爬的一些应对方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-post-series"><div class="item-headline"><i class="fa-solid fa-layer-group"></i><span>系列文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/18/python-crawer/" title="使用 Python 做数据采集(爬虫)">使用 Python 做数据采集(爬虫)</a><time datetime="2023-05-18T05:47:07.000Z" title="发表于 2023-05-18 13:47:07">2023-05-18</time></div></div></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/08/15/vue3-quick/" title="vue3快速上手（尚硅谷b站）"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/08/15/vue3-quick/logo.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vue3快速上手（尚硅谷b站）"/></a><div class="content"><a class="title" href="/2023/08/15/vue3-quick/" title="vue3快速上手（尚硅谷b站）">vue3快速上手（尚硅谷b站）</a><time datetime="2023-08-14T16:00:00.000Z" title="发表于 2023-08-15 00:00:00">2023-08-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/06/redis-basic/" title="redis 基础"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/08/06/redis-basic/logo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="redis 基础"/></a><div class="content"><a class="title" href="/2023/08/06/redis-basic/" title="redis 基础">redis 基础</a><time datetime="2023-08-05T16:00:00.000Z" title="发表于 2023-08-06 00:00:00">2023-08-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/05/transactions-mysql/" title="mysql事务原理"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/08/05/transactions-mysql/logo.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="mysql事务原理"/></a><div class="content"><a class="title" href="/2023/08/05/transactions-mysql/" title="mysql事务原理">mysql事务原理</a><time datetime="2023-08-04T16:00:00.000Z" title="发表于 2023-08-05 00:00:00">2023-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/05/transactions-basic/" title="事务详解"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/08/05/transactions-basic/logo.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="事务详解"/></a><div class="content"><a class="title" href="/2023/08/05/transactions-basic/" title="事务详解">事务详解</a><time datetime="2023-08-04T16:00:00.000Z" title="发表于 2023-08-05 00:00:00">2023-08-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/23/java-memory-optimize/" title="JVM内存调优"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2023/06/23/java-memory-optimize/logo.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JVM内存调优"/></a><div class="content"><a class="title" href="/2023/06/23/java-memory-optimize/" title="JVM内存调优">JVM内存调优</a><time datetime="2023-06-22T16:00:00.000Z" title="发表于 2023-06-23 00:00:00">2023-06-23</time></div></div></div></div></div></div></main><footer id="footer" style="background: none"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Calico</div><div class="footer_custom_text"><a href="icp"><span>Create By hexo,butterfly</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.min.js').then(runMermaid)
  }

  btf.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(()=>{
  const loadLivere = () => {
    if (typeof LivereTower === 'object') window.LivereTower.init()
    else {
      (function(d, s) {
          var j, e = d.getElementsByTagName(s)[0];
          if (typeof LivereTower === 'function') { return; }
          j = d.createElement(s);
          j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
          j.async = true;
          e.parentNode.insertBefore(j, e);
      })(document, 'script');
    }
  }

  if ('Livere' === 'Livere' || !false) {
    if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
    else loadLivere()
  } else {
    window.loadOtherComment = loadLivere
  }
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>