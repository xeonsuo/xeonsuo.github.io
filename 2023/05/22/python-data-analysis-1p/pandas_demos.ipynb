{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53d1c16-a658-4a14-8cfa-f7bcaeb85e8e",
   "metadata": {},
   "source": [
    "### 创建Series对象\n",
    "\n",
    "Pandas 库中的`Series`对象可以用来表示一维数据结构，但是多了索引和一些额外的功能。\n",
    "`Series`类型的内部结构包含了两个数组，其中一个用来保存数据，另一个用来保存数据的索引。我们可以通过列表或数组创建`Series`对象。\n",
    "\n",
    "下面是一些series对象常用操作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6981642-390d-4216-aa23-b5d348da54e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser1:\n",
      " 一季度    120\n",
      "二季度    380\n",
      "三季度    250\n",
      "四季度    360\n",
      "dtype: int64\n",
      "ser2:\n",
      " 一季度    320\n",
      "二季度    180\n",
      "三季度    300\n",
      "四季度    405\n",
      "dtype: int64\n",
      "ser1 + 10 运算:\n",
      " 一季度    130\n",
      "二季度    390\n",
      "三季度    260\n",
      "四季度    370\n",
      "dtype: int64\n",
      "矢量运算 ser1+ser2：\n",
      " 一季度    440\n",
      "二季度    560\n",
      "三季度    550\n",
      "四季度    765\n",
      "dtype: int64\n",
      "ser[1]修改后: 330\n",
      "索引切片ser1[1:3]:\n",
      " 一季度    120\n",
      "二季度    330\n",
      "dtype: int64\n",
      "花式索引:ser2[[\"二季度\",\"三季度\"]]:\n",
      " 二季度    180\n",
      "三季度    300\n",
      "dtype: int64\n",
      "ser2 大于300的 一季度    320\n",
      "四季度    405\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#常规创建方式\n",
    "ser1 = pd.Series(data=[120, 380, 250, 360], index=['一季度', '二季度', '三季度', '四季度'])\n",
    "# `Series`构造器中的`data`参数表示数据，`index`参数表示数据的索引，相当于数据对应的标签。\n",
    "print('ser1:\\n',ser1)\n",
    "\n",
    "# 通过字典创建\n",
    "ser2 = pd.Series({'一季度': 320, '二季度': 180, '三季度': 300, '四季度': 405})\n",
    "print('ser2:\\n',ser2)\n",
    "\n",
    "# series 对象运算\n",
    "\n",
    "#标量运算\n",
    "print('ser1 + 10 运算:\\n',ser1+10)\n",
    "\n",
    "#矢量运算  ser1+ser2\n",
    "print('矢量运算 ser1+ser2：\\n',ser1+ser2)\n",
    "\n",
    "#索引操作\n",
    "ser1['二季度']=330\n",
    "print('ser[1]修改后:',ser1['二季度'])\n",
    "#index 切片\n",
    "print('索引切片ser1[1:3]:\\n',ser1[0:2])\n",
    "# 花式索引\n",
    "print('花式索引:ser2[[\"二季度\",\"三季度\"]]:\\n',ser2[[\"二季度\",\"三季度\"]])\n",
    "\n",
    "print('ser2 大于300的:\\n',ser2[ser2>300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e6815e-804f-4ff8-ae85-b3f6d6e742c6",
   "metadata": {},
   "source": [
    "### Series对象的属性和方法\n",
    "\n",
    "`Series`对象的属性和方法非常多，我们就捡着重要的跟大家讲吧。先看看下面的表格，它展示了`Series`对象常用的属性。\n",
    "\n",
    "| 属性                      | 说明                                                     |\n",
    "|:------------------------- |:-------------------------------------------------------- |\n",
    "| `dtype` / `dtypes`        | 返回`Series`对象的数据类型                               |\n",
    "| `hasnans`                 | 判断`Series`对象中有没有空值                             |\n",
    "| `at` / `iat`              | 通过索引访问`Series`对象中的单个值                       |\n",
    "| `loc` / `iloc`            | 通过索引访问`Series`对象中的单个值或一组值               |\n",
    "| `index`                   | 返回`Series`对象的索引（`Index`对象）                    |\n",
    "| `is_monotonic`            | 判断`Series`对象中的数据是否单调                         |\n",
    "| `is_monotonic_increasing` | 判断`Series`对象中的数据是否单调递增                     |\n",
    "| `is_monotonic_decreasing` | 判断`Series`对象中的数据是否单调递减                     |\n",
    "| `is_unique`               | 判断`Series`对象中的数据是否独一无二                     |\n",
    "| `size`                    | 返回`Series`对象中元素的个数                             |\n",
    "| `values`                  | 以`ndarray`的方式返回`Series`对象中的值（`ndarray`对象） |\n",
    "\n",
    "我们可以通过下面的代码来了解`Series`对象的属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a41cb36e-be56-453f-a744-fb120c17890b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser1:\n",
      " 一季度    120\n",
      "二季度    380\n",
      "三季度    250\n",
      "四季度    360\n",
      "dtype: int64\n",
      "ser2:\n",
      " 一季度    320\n",
      "二季度    180\n",
      "三季度    300\n",
      "四季度    405\n",
      "dtype: int64\n",
      "int64\n",
      "False\n",
      "Index(['一季度', '二季度', '三季度', '四季度'], dtype='object')\n",
      "[320 180 300 405]\n",
      "False\n",
      "True\n",
      "4\n",
      "1205\n",
      "301.25\n",
      "310.0\n",
      "405\n",
      "180\n",
      "92.76987657639737\n",
      "8606.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ser1 = pd.Series(data=[120, 380, 250, 360], index=['一季度', '二季度', '三季度', '四季度'])\n",
    "print('ser1:\\n',ser1)\n",
    "ser2 = pd.Series({'一季度': 320, '二季度': 180, '三季度': 300, '四季度': 405})\n",
    "print('ser2:\\n',ser2)\n",
    "\n",
    "print(ser2.dtype)                    # 数据类型\n",
    "print(ser2.hasnans)                  # 有没有空值\n",
    "print(ser2.index)                    # 索引\n",
    "print(ser2.values)                   # 值\n",
    "print(ser2.is_monotonic_increasing)  # 是否单调递增\n",
    "print(ser2.is_unique)                # 是否每个值都独一无二\n",
    "\n",
    "#统计相关\n",
    "print(ser2.count())   # 计数\n",
    "print(ser2.sum())     # 求和\n",
    "print(ser2.mean())    # 求平均\n",
    "print(ser2.median())  # 找中位数\n",
    "print(ser2.max())     # 找最大\n",
    "print(ser2.min())     # 找最小\n",
    "print(ser2.std())     # 求标准差\n",
    "print(ser2.var())     # 求方差\n",
    "\n",
    "# 总体描述\n",
    "ser2.describe()\n",
    "\n",
    "#统计各个值的出现数量\n",
    "ser3 = pd.Series(data=['apple', 'banana', 'apple', 'pitaya', 'apple', 'pitaya', 'durian'])\n",
    "ser3.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054560b-e71b-4b10-a5b8-98a26ca4eec8",
   "metadata": {},
   "source": [
    "#### 处理数据\n",
    "\n",
    "`Series`对象的`isna()`和`isnull()`方法可以用于空值的判断，`notna()`和`notnull()`方法可以用于非空值的判断，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser4 = pd.Series(data=[10, 20, np.nan, 30, np.nan])\n",
    "ser4.isna()\n",
    "```\n",
    "\n",
    "> **说明**：`np.nan`是一个IEEE 754标准的浮点小数，专门用来表示“不是一个数”，在上面的代码中我们用它来代表空值；当然，也可以用 Python 中的`None`来表示空值，在 pandas 中`None`也会被处理为`np.nan`。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    False\n",
    "1    False\n",
    "2     True\n",
    "3    False\n",
    "4     True\n",
    "dtype: bool\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser4.notna()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0     True\n",
    "1     True\n",
    "2    False\n",
    "3     True\n",
    "4    False\n",
    "dtype: bool\n",
    "```\n",
    "\n",
    "`Series`对象的`dropna()`和`fillna()`方法分别用来删除空值和填充空值，具体的用法如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser4.dropna()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    10.0\n",
    "1    20.0\n",
    "3    30.0\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser4.fillna(value=40)  # 将空值填充为40\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    10.0\n",
    "1    20.0\n",
    "2    40.0\n",
    "3    30.0\n",
    "4    40.0\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser4.fillna(method='ffill')  # 用空值前面的非空值填充\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    10.0\n",
    "1    20.0\n",
    "2    20.0\n",
    "3    30.0\n",
    "4    30.0\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "需要提醒大家注意的是，`dropna()`和`fillna()`方法都有一个名为`inplace`的参数，它的默认值是`False`，表示删除空值或填充空值不会修改原来的`Series`对象，而是返回一个新的`Series`对象。如果将`inplace`参数的值修改为`True`，那么删除或填充空值会就地操作，直接修改原来的`Series`对象，此时方法的返回值是`None`。后面我们会接触到的很多方法，包括`DataFrame`对象的很多方法都会有这个参数，它们的意义跟这里是一样的。\n",
    "\n",
    "\n",
    "`Series`对象的`mask()`和`where()`方法可以将满足或不满足条件的值进行替换，如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser5 = pd.Series(range(5))\n",
    "ser5.where(ser5 > 0)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    NaN\n",
    "1    1.0\n",
    "2    2.0\n",
    "3    3.0\n",
    "4    4.0\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser5.where(ser5 > 1, 10)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    10\n",
    "1    10\n",
    "2     2\n",
    "3     3\n",
    "4     4\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser5.mask(ser5 > 1, 10)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0     0\n",
    "1     1\n",
    "2    10\n",
    "3    10\n",
    "4    10\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "`Series`对象的`duplicated()`方法可以帮助我们找出重复的数据，而`drop_duplicates()`方法可以帮我们删除重复数据。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser3.duplicated()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    False\n",
    "1    False\n",
    "2     True\n",
    "3    False\n",
    "4     True\n",
    "5     True\n",
    "6    False\n",
    "dtype: bool\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser3.drop_duplicates()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0     apple\n",
    "1    banana\n",
    "3    pitaya\n",
    "6    durian\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "`Series`对象的`apply()`和`map()`方法非常重要，它们可以通过字典或者指定的函数来处理数据，把数据映射或转换成我们想要的样子。这两个方法在数据准备阶段非常重要，我们先来试一试这个名为`map`的方法。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser6 = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n",
    "ser6\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0       cat\n",
    "1       dog\n",
    "2       NaN\n",
    "3    rabbit\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser6.map({'cat': 'kitten', 'dog': 'puppy'})\n",
    "```\n",
    "\n",
    "> **说明**：通过字典给出的映射规则对数据进行处理。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    kitten\n",
    "1     puppy\n",
    "2       NaN\n",
    "3       NaN\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser6.map('I am a {}'.format, na_action='ignore')\n",
    "```\n",
    "\n",
    "> **说明**：将指定字符串的`format`方法作用到数据系列的数据上，忽略掉所有的空值。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0       I am a cat\n",
    "1       I am a dog\n",
    "2              NaN\n",
    "3    I am a rabbit\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "我们创建一个新的`Series`对象，\n",
    "\n",
    "```Python\n",
    "ser7 = pd.Series([20, 21, 12],  index=['London', 'New York', 'Helsinki'])\n",
    "ser7\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "London      20\n",
    "New York    21\n",
    "Helsinki    12\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser7.apply(np.square)\n",
    "```\n",
    "\n",
    "> **说明**：将求平方的函数作用到数据系列的数据上，也可以将参数`np.square`替换为`lambda x: x ** 2`。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "London      400\n",
    "New York    441\n",
    "Helsinki    144\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser7.apply(lambda x, value: x - value, args=(5, ))\n",
    "```\n",
    "\n",
    "> 注意：上面`apply`方法中的`lambda`函数有两个参数，第一个参数是数据系列中的数据，而第二个参数需要我们传入，所以我们给`apply`方法增加了`args`参数，用于给`lambda`函数的第二个参数传值。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "London      15\n",
    "New York    16\n",
    "Helsinki     7\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "#### 取头部值和排序\n",
    "\n",
    "`Series`对象的`sort_index()`和`sort_values()`方法可以用于对索引和数据的排序，排序方法有一个名为`ascending`的布尔类型参数，该参数用于控制排序的结果是升序还是降序；而名为`kind`的参数则用来控制排序使用的算法，默认使用了`quicksort`，也可以选择`mergesort`或`heapsort`；如果存在空值，那么可以用`na_position`参数空值放在最前还是最后，默认是`last`，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser8 = pd.Series(\n",
    "    data=[35, 96, 12, 57, 25, 89], \n",
    "    index=['grape', 'banana', 'pitaya', 'apple', 'peach', 'orange']\n",
    ")\n",
    "ser8.sort_values()  # 按值从小到大排序\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "pitaya    12\n",
    "peach     25\n",
    "grape     35\n",
    "apple     57\n",
    "orange    89\n",
    "banana    96\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser8.sort_index(ascending=False)  # 按索引从大到小排序\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "pitaya    12\n",
    "peach     25\n",
    "orange    89\n",
    "grape     35\n",
    "banana    96\n",
    "apple     57\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "如果要从`Series`对象中找出元素中最大或最小的“Top-N”，我们不需要对所有的值进行排序的，可以使用`nlargest()`和`nsmallest()`方法来完成，如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser8.nlargest(3)  # 值最大的3个\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "banana    96\n",
    "orange    89\n",
    "apple     57\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "ser8.nsmallest(2)  # 值最小的2个\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "pitaya    12\n",
    "peach     25\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 绘制图表\n",
    "\n",
    "`Series`对象有一个名为`plot`的方法可以用来生成图表，如果选择生成折线图、饼图、柱状图等，默认会使用`Series`对象的索引作为横坐标，使用`Series`对象的数据作为纵坐标。下面我们创建一个`Series`对象并基于它绘制柱状图，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ser9 = pd.Series({'Q1': 400, 'Q2': 520, 'Q3': 180, 'Q4': 380})\n",
    "# 通过plot方法的kind指定图表类型为柱状图\n",
    "ser9.plot(kind='bar')\n",
    "# 定制纵轴的取值范围\n",
    "plt.ylim(0, 600)\n",
    "# 定制横轴刻度（旋转到0度）\n",
    "plt.xticks(rotation=0)\n",
    "# 为柱子增加数据标签\n",
    "for i in range(ser9.size):\n",
    "    plt.text(i, ser9[i] + 5, ser9[i], ha='center')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/ser_bar_graph.png\" style=\"zoom:35%;\">\n",
    "\n",
    "我们也可以将其绘制为饼图，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "# plot方法的kind参数指定了图表类型为饼图\n",
    "# autopct会自动计算并显示百分比\n",
    "# pctdistance用来控制百分比到圆心的距离\n",
    "ser9.plot(kind='pie', autopct='%.1f%%', pctdistance=0.65)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/ser_pie_graph.png\" style=\"zoom:35%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff5ecd-e6dd-4947-a81e-f58ef48b0b92",
   "metadata": {},
   "source": [
    "### DataFrame对象\n",
    "\n",
    "如果使用 pandas 做数据分析，那么`DataFrame`一定是被使用得最多的类型，它可以用来保存和处理异质的二维数据。这里所谓的“异质”是指`DataFrame`中每个列的数据类型不需要相同，这也是它区别于 NumPy 二维数组的地方。`DataFrame`提供了极为丰富的属性和方法，帮助我们实现对数据的重塑、清洗、预处理、透视、呈现等一系列操作。\n",
    "\n",
    "创建DataFrame对象\n",
    "通过二维数组创建DataFrame对象\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "887c3bbb-2a5d-498d-b526-56af57392a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      语文  数学  英语\n",
      "1001  99  81  63\n",
      "1002  78  78  80\n",
      "1003  69  65  70\n",
      "1004  83  67  92\n",
      "1005  62  73  74\n",
      "      语文  数学  英语\n",
      "1001  62  95  66\n",
      "1002  72  65  75\n",
      "1003  93  86  82\n",
      "1004  88  66  69\n",
      "1005  93  87  82\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "scores = np.random.randint(60, 101, (5, 3))\n",
    "courses = ['语文', '数学', '英语']\n",
    "stu_ids = np.arange(1001, 1006)\n",
    "df1 = pd.DataFrame(data=scores, columns=courses, index=stu_ids)\n",
    "print(df1)\n",
    "\n",
    "# 通过字典创建\n",
    "scores = {\n",
    "    '语文': [62, 72, 93, 88, 93],\n",
    "    '数学': [95, 65, 86, 66, 87],\n",
    "    '英语': [66, 75, 82, 69, 82],\n",
    "}\n",
    "stu_ids = np.arange(1001, 1006)\n",
    "df2 = pd.DataFrame(data=scores, index=stu_ids)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b4d74-3c36-4a76-a370-43b7fb785063",
   "metadata": {},
   "source": [
    "#### 读取CSV文件创建DataFrame对象\n",
    "\n",
    "可以通过`pandas` 模块的`read_csv`函数来读取 CSV 文件，`read_csv`函数的参数非常多，下面介绍几个比较重要的参数。\n",
    "\n",
    "- `sep` / `delimiter`：分隔符，默认是`,`。\n",
    "- `header`：表头（列索引）的位置，默认值是`infer`，用第一行的内容作为表头（列索引）。\n",
    "- `index_col`：用作行索引（标签）的列。\n",
    "- `usecols`：需要加载的列，可以使用序号或者列名。\n",
    "- `true_values` / `false_values`：哪些值被视为布尔值`True` / `False`。\n",
    "- `skiprows`：通过行号、索引或函数指定需要跳过的行。\n",
    "- `skipfooter`：要跳过的末尾行数。\n",
    "- `nrows`：需要读取的行数。\n",
    "- `na_values`：哪些值被视为空值。\n",
    "- `iterator`：设置为`True`，函数返回迭代器对象。\n",
    "- `chunksize`：配合上面的参数，设置每次迭代获取的数据体量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03f18d16-d65e-4678-84e0-22bc362f6d4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>birthday</th>\n",
       "      <th>company</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>杨效丰</td>\n",
       "      <td>1972-12</td>\n",
       "      <td>北京利德华福电气技术有限公司</td>\n",
       "      <td>122.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>纪丰伟</td>\n",
       "      <td>1974-12</td>\n",
       "      <td>北京航天数据股份有限公司</td>\n",
       "      <td>121.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>王永</td>\n",
       "      <td>1974-05</td>\n",
       "      <td>品牌联盟(北京)咨询股份公司</td>\n",
       "      <td>118.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>杨静</td>\n",
       "      <td>1975-07</td>\n",
       "      <td>中科专利商标代理有限责任公司</td>\n",
       "      <td>118.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>张凯江</td>\n",
       "      <td>1974-11</td>\n",
       "      <td>北京阿里巴巴云计算技术有限公司</td>\n",
       "      <td>117.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6015</th>\n",
       "      <td>孙宏波</td>\n",
       "      <td>1978-08</td>\n",
       "      <td>华为海洋网络有限公司北京科技分公司</td>\n",
       "      <td>90.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6016</th>\n",
       "      <td>刘丽香</td>\n",
       "      <td>1976-11</td>\n",
       "      <td>福斯（上海）流体设备有限公司北京分公司</td>\n",
       "      <td>90.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>周崧</td>\n",
       "      <td>1977-10</td>\n",
       "      <td>赢创德固赛（中国）投资有限公司</td>\n",
       "      <td>90.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6018</th>\n",
       "      <td>赵妍</td>\n",
       "      <td>1979-07</td>\n",
       "      <td>澳科利耳医疗器械（北京）有限公司</td>\n",
       "      <td>90.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6019</th>\n",
       "      <td>贺锐</td>\n",
       "      <td>1981-06</td>\n",
       "      <td>北京宝洁技术有限公司</td>\n",
       "      <td>90.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6019 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     name birthday              company   score\n",
       "id                                             \n",
       "1     杨效丰  1972-12       北京利德华福电气技术有限公司  122.59\n",
       "2     纪丰伟  1974-12         北京航天数据股份有限公司  121.25\n",
       "3      王永  1974-05       品牌联盟(北京)咨询股份公司  118.96\n",
       "4      杨静  1975-07       中科专利商标代理有限责任公司  118.21\n",
       "5     张凯江  1974-11      北京阿里巴巴云计算技术有限公司  117.79\n",
       "...   ...      ...                  ...     ...\n",
       "6015  孙宏波  1978-08    华为海洋网络有限公司北京科技分公司   90.75\n",
       "6016  刘丽香  1976-11  福斯（上海）流体设备有限公司北京分公司   90.75\n",
       "6017   周崧  1977-10      赢创德固赛（中国）投资有限公司   90.75\n",
       "6018   赵妍  1979-07     澳科利耳医疗器械（北京）有限公司   90.75\n",
       "6019   贺锐  1981-06           北京宝洁技术有限公司   90.75\n",
       "\n",
       "[6019 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df3 = pd.read_csv('data/2018年北京积分落户数据.csv', index_col='id')\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ca780-8368-4286-a154-47f7b58ab261",
   "metadata": {},
   "source": [
    "#### 读取Excel工作表创建DataFrame对象\n",
    "\n",
    "可以通过`pandas` 模块的`read_excel`函数来读取 Excel 文件，该函数与上面的`read_csv`非常类似，\n",
    "多了一个`sheet_name`参数来指定数据表的名称，但是不同于 CSV 文件，没有`sep`或`delimiter`这样的参数。\n",
    "假设有名为“2022年股票数据.xlsx”的 Excel 文件，里面有用股票代码命名的五个表单，分别是阿里巴巴（BABA）、百度（BIDU）、京东（JD）、亚马逊（AMZN）、甲骨文（ORCL）这五个公司2022年的股票数据，如果想加载亚马逊的股票数据，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e93f235-1969-4283-b36e-0d838704496b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>83.120</td>\n",
       "      <td>84.050</td>\n",
       "      <td>82.4700</td>\n",
       "      <td>84.000</td>\n",
       "      <td>62401194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>82.870</td>\n",
       "      <td>84.550</td>\n",
       "      <td>82.5500</td>\n",
       "      <td>84.180</td>\n",
       "      <td>54995895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>82.800</td>\n",
       "      <td>83.480</td>\n",
       "      <td>81.6900</td>\n",
       "      <td>81.820</td>\n",
       "      <td>58228575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>84.970</td>\n",
       "      <td>85.350</td>\n",
       "      <td>83.0000</td>\n",
       "      <td>83.040</td>\n",
       "      <td>57284035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-23</th>\n",
       "      <td>83.250</td>\n",
       "      <td>85.780</td>\n",
       "      <td>82.9344</td>\n",
       "      <td>85.250</td>\n",
       "      <td>57433655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>163.839</td>\n",
       "      <td>165.243</td>\n",
       "      <td>162.0310</td>\n",
       "      <td>162.554</td>\n",
       "      <td>46605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>163.450</td>\n",
       "      <td>164.800</td>\n",
       "      <td>161.9370</td>\n",
       "      <td>163.254</td>\n",
       "      <td>51957780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>166.883</td>\n",
       "      <td>167.126</td>\n",
       "      <td>164.3570</td>\n",
       "      <td>164.357</td>\n",
       "      <td>64302720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>170.438</td>\n",
       "      <td>171.400</td>\n",
       "      <td>166.3490</td>\n",
       "      <td>167.522</td>\n",
       "      <td>70725160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>167.550</td>\n",
       "      <td>170.704</td>\n",
       "      <td>166.1600</td>\n",
       "      <td>170.404</td>\n",
       "      <td>63869140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open     High       Low    Close    Volume\n",
       "Date                                                     \n",
       "2022-12-30   83.120   84.050   82.4700   84.000  62401194\n",
       "2022-12-29   82.870   84.550   82.5500   84.180  54995895\n",
       "2022-12-28   82.800   83.480   81.6900   81.820  58228575\n",
       "2022-12-27   84.970   85.350   83.0000   83.040  57284035\n",
       "2022-12-23   83.250   85.780   82.9344   85.250  57433655\n",
       "...             ...      ...       ...      ...       ...\n",
       "2022-01-07  163.839  165.243  162.0310  162.554  46605900\n",
       "2022-01-06  163.450  164.800  161.9370  163.254  51957780\n",
       "2022-01-05  166.883  167.126  164.3570  164.357  64302720\n",
       "2022-01-04  170.438  171.400  166.3490  167.522  70725160\n",
       "2022-01-03  167.550  170.704  166.1600  170.404  63869140\n",
       "\n",
       "[251 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df4 = pd.read_excel('data/2022年股票数据.xlsx', sheet_name='AMZN', index_col='Date')\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf2d88-4745-4ace-86db-0fe9ac1d0abc",
   "metadata": {},
   "source": [
    "#### 读取关系数据库二维表创建DataFrame对象\n",
    "\n",
    "`pandas`模块的`read_sql`函数可以通过 SQL 语句从数据库中读取数据创建`DataFrame`对象，\n",
    "该函数的第二个参数代表了需要连接的数据库。对于 MySQL 数据库，我们可以通过`pymysql`或`mysqlclient`来创建数据库连接（需要提前安装好三方库），得到一个`Connection` 对象，而这个对象就是`read_sql`函数需要的第二个参数，代码如下所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "689d631a-455a-41bd-80d8-a345e95c642a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ename   job     mgr   sal    comm  dno\n",
      "eno                                        \n",
      "1359   胡一刀   销售员  3344.0  1800   200.0   30\n",
      "2056    乔峰   分析师  7800.0  5000  1500.0   20\n",
      "3088   李莫愁   设计师  2056.0  3500   800.0   20\n",
      "3211   张无忌   程序员  2056.0  3200     NaN   20\n",
      "3233   丘处机   程序员  2056.0  3400     NaN   20\n",
      "3244   欧阳锋   程序员  3088.0  3200     NaN   20\n",
      "3251   张翠山   程序员  2056.0  4000     NaN   20\n",
      "3344    黄蓉  销售主管  7800.0  3000   800.0   30\n",
      "3577    杨过    会计  5566.0  2200     NaN   10\n",
      "3588   朱九真    会计  5566.0  2500     NaN   10\n",
      "4466   苗人凤   销售员  3344.0  2500     NaN   30\n",
      "5234    郭靖    出纳  5566.0  2000     NaN   10\n",
      "5566   宋远桥   会计师  7800.0  4000  1000.0   10\n",
      "7800   张三丰    总裁     NaN  9000  1200.0   20\n",
      "    dname dloc\n",
      "dno           \n",
      "10    会计部   北京\n",
      "20    研发部   成都\n",
      "30    销售部   重庆\n",
      "40    运维部   深圳\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "# 创建一个MySQL数据库的连接对象\n",
    "#conn = pymysql.connect(\n",
    "#    host='101.42.16.8', port=3306,\n",
    "#    user='guest', password='Guest.618',\n",
    "#    database='hrs', charset='utf8mb4'\n",
    "#)\n",
    "# 通过SQL从数据库二维表读取数据创建DataFrame\n",
    "#df5 = pd.read_sql('select * from tb_emp', conn, index_col='eno')\n",
    "#df5\n",
    "\n",
    "# 通过指定的URL（统一资源定位符）访问数据库\n",
    "engine = create_engine('mysql+pymysql://guest:Guest.618@101.42.16.8:3306/hrs')\n",
    "# 直接通过表名加载整张表的数据\n",
    "df6 = pd.read_sql('tb_emp', engine, index_col='eno')\n",
    "print(df6)\n",
    "\n",
    "df7 = pd.read_sql('select dno, dname, dloc from tb_dept', engine, index_col='dno')\n",
    "print(df7)\n",
    "engine.connect().close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60311b-4f38-4914-aeaa-69ea4da27b7b",
   "metadata": {},
   "source": [
    "#### 基本属性和方法\n",
    "\n",
    "在开始讲解DataFrame的属性和方法前，我们先从之前提到的hrs数据库中读取三张表的数据，创建出三个DataFrame对象，完整的代码如下所示。\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "``` python\n",
    "engine = create_engine('mysql+pymysql://guest:Guest.618@101.42.16.8:3306/hrs')\n",
    "dept_df = pd.read_sql_table('tb_dept', engine, index_col='dno')\n",
    "emp_df = pd.read_sql_table('tb_emp', engine, index_col='eno')\n",
    "emp2_df = pd.read_sql_table('tb_emp2', engine, index_col='eno')\n",
    "```\n",
    "得到的三个DataFrame对象如下所示。\n",
    "\n",
    "部门表（dept_df），其中dno是部门的编号，dname和dloc分别是部门的名称和所在地。\n",
    "```\n",
    "    dname  dloc\n",
    "dno\n",
    "10  会计部 北京\n",
    "20  研发部 成都\n",
    "30  销售部 重庆\n",
    "40  运维部 深圳\n",
    "员工表（emp_df），其中eno是员工编号，ename、job、mgr、sal、comm和dno分别代表员工的姓名、职位、主管编号、月薪、补贴和部门编号。\n",
    "\n",
    "|id|ename|job||mgr|sal|comm|dno\n",
    "\n",
    "eno\n",
    "1359    胡一刀    销售员     3344.0   1800    200.0   30\n",
    "2056    乔峰      分析师     7800.0   5000    1500.0  20\n",
    "3088    李莫愁    设计师     2056.0   3500    800.0   20\n",
    "3211    张无忌    程序员     2056.0   3200    NaN     20\n",
    "3233    丘处机    程序员     2056.0   3400    NaN     20\n",
    "3244    欧阳锋    程序员     3088.0   3200    NaN     20\n",
    "3251    张翠山    程序员     2056.0   4000    NaN     20\n",
    "3344    黄蓉      销售主管   7800.0   3000    800.0   30\n",
    "3577    杨过      会计       5566.0   2200    NaN     10\n",
    "3588    朱九真    会计       5566.0   2500    NaN     10\n",
    "4466    苗人凤    销售员     3344.0   2500    NaN     30\n",
    "5234    郭靖      出纳       5566.0   2000    NaN     10\n",
    "5566    宋远桥    会计师     7800.0   4000    1000.0  10\n",
    "7800    张三丰    总裁       NaN      9000    1200.0  20\n",
    "```\n",
    "说明：在数据库中mgr和comm两个列的数据类型是int，但是因为有缺失值（空值），读取到DataFrame之后，列的数据类型变成了float，因为我们通常会用float类型的NaN来表示空值。\n",
    "\n",
    "员工表（emp2_df），跟上面的员工表结构相同，但是保存了不同的员工数据\n",
    "\n",
    "```\n",
    "       ename    job      mgr      sal    comm    dno\n",
    "eno                                      \n",
    "9500   张三丰   总裁      NaN      50000  8000    20\n",
    "9600   王大锤   程序员    9800.0   8000   600     20\n",
    "9700   张三丰   总裁      NaN      60000  6000    20\n",
    "9800   骆昊     架构师    7800.0   30000  5000    20\n",
    "9900   陈小刀   分析师    9800.0   10000  1200    20\n",
    "\n",
    "\n",
    "\n",
    "DataFrame对象的属性如下表所示。\n",
    "\n",
    "属性名\t说明\n",
    "at / iat\t通过标签获取DataFrame中的单个值。\n",
    "columns\tDataFrame对象列的索引\n",
    "dtypes\tDataFrame对象每一列的数据类型\n",
    "empty\tDataFrame对象是否为空\n",
    "loc / iloc\t通过标签获取DataFrame中的一组值。\n",
    "ndim\tDataFrame对象的维度\n",
    "shape\tDataFrame对象的形状（行数和列数）\n",
    "size\tDataFrame对象中元素的个数\n",
    "values\tDataFrame对象的数据对应的二维数组\n",
    "关于DataFrame的方法，首先需要了解的是info()方法，它可以帮助我们了解DataFrame的相关信息，如下所示。\n",
    "```\n",
    "代码：\n",
    "```\n",
    "emp_df.info()\n",
    "```\n",
    "输出：\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 14 entries, 1359 to 7800\n",
    "Data columns (total 6 columns):\n",
    " #   Column  Non-Null Count  Dtype  \n",
    "---  ------  --------------  -----  \n",
    " 0   ename   14 non-null     object \n",
    " 1   job     14 non-null     object \n",
    " 2   mgr     13 non-null     float64\n",
    " 3   sal     14 non-null     int64  \n",
    " 4   comm    6 non-null      float64\n",
    " 5   dno     14 non-null     int64  \n",
    "dtypes: float64(2), int64(2), object(2)\n",
    "memory usage: 1.3+ KB\n",
    "\n",
    "\n",
    "如果需要查看DataFrame的头部或尾部的数据，可以使用head()或tail()方法，这两个方法的默认参数是5，表示获取DataFrame最前面5行或最后面5行的数据，如下所示。\n",
    "emp_df.head()\n",
    "输出：\n",
    "\n",
    "        ename    job    mgr    sal    comm  dno\n",
    "eno                     \n",
    "1359    胡一刀   销售员   3344   1800  200   30\n",
    "2056    乔峰     分析师   7800   5000  1500  20\n",
    "3088    李莫愁   设计师   2056   3500  800   20\n",
    "3211    张无忌   程序员   2056   3200  NaN   20\n",
    "3233    丘处机   程序员   2056   3400  NaN   20\n",
    "\n",
    "\n",
    "操作数据\n",
    "\n",
    "索引和切片\n",
    "如果要获取DataFrame的某一列，例如取出上面emp_df的ename列，可以使用下面的两种方式。\n",
    "\n",
    "emp_df.ename\n",
    "或者\n",
    "\n",
    "emp_df['ename']\n",
    "执行上面的代码可以发现，我们获得的是一个Series对象。事实上，DataFrame对象就是将多个Series对象组合到一起的结果。\n",
    "\n",
    "如果要获取DataFrame的某一行，可以使用整数索引或我们设置的索引，例如取出员工编号为2056的员工数据，代码如下所示。\n",
    "\n",
    "emp_df.iloc[1]\n",
    "或者\n",
    "\n",
    "emp_df.loc[2056]\n",
    "通过执行上面的代码我们发现，单独取DataFrame 的某一行或某一列得到的都是Series对象。我们当然也可以通过花式索引来获取多个行或多个列的数据，花式索引的结果仍然是一个DataFrame对象。\n",
    "\n",
    "获取多个列：\n",
    "\n",
    "emp_df[['ename', 'job']]\n",
    "获取多个行：\n",
    "\n",
    "emp_df.loc[[2056, 7800, 3344]]\n",
    "如果要获取或修改DataFrame 对象某个单元格的数据，需要同时指定行和列的索引，例如要获取员工编号为2056的员工的职位信息，代码如下所示。\n",
    "\n",
    "emp_df['job'][2056]\n",
    "python\n",
    "或者\n",
    "\n",
    "emp_df.loc[2056]['job']\n",
    "或者\n",
    "\n",
    "emp_df.loc[2056, 'job']\n",
    "我们推荐大家使用第三种做法，因为它只做了一次索引运算。如果要将该员工的职位修改为“架构师”，可以使用下面的代码。\n",
    "\n",
    "emp_df.loc[2056, 'job'] = '架构师'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "当然，我们也可以通过切片操作来获取多行多列，相信大家一定已经想到了这一点。\n",
    "\n",
    "emp_df.loc[2056:3344]\n",
    "输出：\n",
    "\n",
    "        ename    job        mgr      sal     comm    dno\n",
    "eno\n",
    "2056    乔峰      分析师     7800.0   5000    1500.0  20\n",
    "3088    李莫愁    设计师     2056.0   3500    800.0   20\n",
    "3211    张无忌    程序员     2056.0   3200    NaN     20\n",
    "3233    丘处机    程序员     2056.0   3400    NaN     20\n",
    "3244    欧阳锋    程序员     3088.0   3200    NaN     20\n",
    "3251    张翠山    程序员     2056.0   4000    NaN     20\n",
    "3344    黄蓉      销售主管   7800.0   3000    800.0   30\n",
    "\n",
    "\n",
    "\n",
    "数据筛选\n",
    "上面我们提到了花式索引，相信大家已经联想到了布尔索引。跟ndarray和Series一样，我们可以通过布尔索引对DataFrame对象进行数据筛选，例如我们要从emp_df中筛选出月薪超过3500的员工，代码如下所示。\n",
    "\n",
    "emp_df[emp_df.sal > 3500]\n",
    "输出：\n",
    "\n",
    "        ename    job        mgr      sal     comm    dno\n",
    "eno\n",
    "2056    乔峰      分析师     7800.0   5000    1500.0  20\n",
    "3251    张翠山    程序员     2056.0   4000    NaN     20\n",
    "5566    宋远桥    会计师     7800.0   4000    1000.0  10\n",
    "7800    张三丰    总裁       NaN      9000    1200.0  20\n",
    "\n",
    "\n",
    "当然，我们也可以组合多个条件来进行数据筛选，例如从emp_df中筛选出月薪超过3500且部门编号为20的员工，代码如下所示。\n",
    "\n",
    "emp_df[(emp_df.sal > 3500) & (emp_df.dno == 20)]\n",
    "输出：\n",
    "\n",
    "        ename    job        mgr      sal     comm    dno\n",
    "eno\n",
    "2056    乔峰      分析师     7800.0   5000    1500.0  20\n",
    "3251    张翠山    程序员     2056.0   4000    NaN     20\n",
    "7800    张三丰    总裁       NaN      9000    1200.0  20\n",
    "\n",
    "\n",
    "除了使用布尔索引，DataFrame对象的query方法也可以实现数据筛选，query方法的参数是一个字符串，它代表了筛选数据使用的表达式，而且更符合 Python 程序员的使用习惯。下面我们使用query方法将上面的效果重新实现一遍，代码如下所示。\n",
    "\n",
    "emp_df.query('sal > 3500 and dno == 20')\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a6cbb-dc97-4c25-9708-f108687ed4a6",
   "metadata": {},
   "source": [
    "### 数据准备（连接、重塑、清洗）\n",
    "\n",
    "    在完成数据加载之后，我们可能需要对事实表和维度表进行连接，这是对数据进行多维度拆解的基础；我们可能从不同的数据源加载了结构相同的数据，我们需要将这些数据拼接起来；我们把这些操作统称为数据重塑。当然，由于企业的信息化水平以及数据中台建设水平的差异，我们拿到的数据未必是质量很好的，可能还需要对数据中的缺失值、重复值、异常值进行适当的处理。即便我们获取的数据在质量上是没有问题的，但也可能需要对数据进行一系列的预处理，才能满足我们做数据分析的需求。接下来，我们就为大家讲解和梳理这方面的知识。\n",
    "\n",
    "#### 数据重塑\n",
    "\n",
    "有的时候，我们做数据分析需要的原始数据可能并不是来自一个地方，就像上一章的例子中，我们从关系型数据库中读取了三张表，得到了三个`DataFrame`对象，但实际工作可能需要我们把他们的数据整合到一起。例如：`emp_df`和`emp2_df`其实都是员工的数据，而且数据结构完全一致，我们可以使用`pandas`提供的`concat`函数实现两个或多个`DataFrame`的数据拼接，代码如下所示。\n",
    "\n",
    "\n",
    "代码如下所示。\n",
    "\n",
    "```Python\n",
    "all_emp_df = pd.concat([emp_df, emp2_df])\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "        ename    job        mgr      sal     comm    dno\n",
    "eno\n",
    "1359    胡一刀    销售员\t   3344.0\t1800\t200.0\t30\n",
    "2056    乔峰\t    分析师\t    7800.0\t 5000\t 1500.0\t 20\n",
    "3088    李莫愁\t   设计师\t   2056.0\t3500\t800.0\t20\n",
    "3211    张无忌\t   程序员\t   2056.0\t3200\tNaN     20\n",
    "3233    丘处机\t   程序员\t   2056.0\t3400\tNaN\t    20\n",
    "3244    欧阳锋\t   程序员\t   3088.0\t3200\tNaN     20\n",
    "3251    张翠山\t   程序员\t   2056.0\t4000\tNaN\t    20\n",
    "3344    黄蓉\t    销售主管   7800.0\t3000\t800.0\t30\n",
    "3577    杨过\t    会计\t     5566.0\t  2200\t  NaN\t  10\n",
    "3588    朱九真\t   会计\t    5566.0\t 2500\t NaN\t 10\n",
    "4466    苗人凤\t   销售员\t   3344.0\t2500\tNaN\t    30\n",
    "5234    郭靖\t    出纳\t     5566.0\t  2000\t  NaN\t  10\n",
    "5566    宋远桥\t   会计师\t   7800.0\t4000\t1000.0\t10\n",
    "7800    张三丰\t   总裁\t    NaN      9000\t 1200.0\t 20\n",
    "9500\t张三丰\t   总裁\t    NaN\t     50000\t 8000.0\t 20\n",
    "9600\t王大锤    程序员\t   9800.0\t8000\t600.0\t20\n",
    "9700\t张三丰\t   总裁\t    NaN\t     60000\t 6000.0\t 20\n",
    "9800\t骆昊\t    架构师\t    7800.0\t 30000\t 5000.0\t 20\n",
    "9900\t陈小刀\t   分析师\t   9800.0\t10000\t1200.0\t20\n",
    "```\n",
    "\n",
    "上面的代码将两个代表员工数据的`DataFrame`拼接到了一起，接下来我们使用`merge`函数将员工表和部门表的数据合并到一张表中，代码如下所示。\n",
    "\n",
    "先使用`reset_index`方法重新设置`all_emp_df`的索引，这样`eno` 不再是索引而是一个普通列，`reset_index`方法的`inplace`参数设置为`True`表示，重置索引的操作直接在`all_emp_df`上执行，而不是返回修改后的新对象。\n",
    "\n",
    "```Python\n",
    "all_emp_df.reset_index(inplace=True)\n",
    "```\n",
    "\n",
    "通过`merge`函数合并数据，当然，也可以调用`DataFrame`对象的`merge`方法来达到同样的效果。\n",
    "\n",
    "```Python\n",
    "pd.merge(all_emp_df, dept_df, how='inner', on='dno')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "    eno\t    ename\tjob\t     mgr\t sal\t comm\t dno\tdname\t dloc\n",
    "0\t1359\t胡一刀\t 销售员\t3344.0\t1800\t200.0\t30\t   销售部\t 重庆\n",
    "1\t3344\t黄蓉\t  销售主管\t7800.0\t3000\t800.0\t30\t   销售部\t 重庆\n",
    "2\t4466\t苗人凤\t 销售员\t3344.0\t2500\tNaN\t    30\t   销售部\t 重庆\n",
    "3\t2056\t乔峰\t  分析师\t 7800.0\t 5000\t 1500.0\t 20\t    研发部\t  成都\n",
    "4\t3088\t李莫愁\t 设计师\t2056.0\t3500\t800.0\t20\t   研发部\t 成都\n",
    "5\t3211\t张无忌  程序员\t2056.0\t3200\tNaN\t    20\t   研发部\t 成都\n",
    "6\t3233\t丘处机\t 程序员\t2056.0\t3400\tNaN\t    20\t   研发部\t 成都\n",
    "7\t3244\t欧阳锋\t 程序员\t3088.0\t3200\tNaN\t    20\t   研发部\t 成都\n",
    "8\t3251\t张翠山\t 程序员\t2056.0\t4000\tNaN\t    20\t   研发部\t 成都\n",
    "9\t7800\t张三丰\t 总裁\t     NaN\t 9000\t 1200.0\t 20\t    研发部\t  成都\n",
    "10\t9500\t张三丰\t 总裁\t     NaN\t 50000\t 8000.0\t 20\t    研发部\t  成都\n",
    "11\t9600\t王大锤\t 程序员\t9800.0\t8000\t600.0\t20\t   研发部\t 成都\n",
    "12\t9700\t张三丰\t 总裁\t     NaN\t 60000\t 6000.0\t 20\t    研发部\t  成都\n",
    "13\t9800\t骆昊\t  架构师\t 7800.0\t 30000\t 5000.0\t 20\t    研发部\t  成都\n",
    "14\t9900\t陈小刀\t 分析师\t9800.0\t10000\t1200.0\t20\t   研发部\t 成都\n",
    "15\t3577\t杨过\t  会计\t  5566.0  2200\t  NaN\t  10\t会计部\t  北京\n",
    "16\t3588\t朱九真\t 会计\t     5566.0\t 2500\t NaN\t 10\t   会计部\t 北京\n",
    "17\t5234\t郭靖\t  出纳\t  5566.0  2000\t  NaN\t  10\t会计部\t  北京\n",
    "18\t5566\t宋远桥\t 会计师\t7800.0\t4000\t1000.0\t10\t  会计部\t北京\n",
    "```\n",
    "\n",
    "`merge`函数的一个参数代表合并的左表、第二个参数代表合并的右表，有SQL编程经验的同学对这两个词是不是感觉到非常亲切。正如大家猜想的那样，`DataFrame`对象的合并跟数据库中的表连接非常类似，所以上面代码中的`how`代表了合并两张表的方式，有`left`、`right`、`inner`、`outer`四个选项；而`on`则代表了基于哪个列实现表的合并，相当于 SQL 表连接中的连表条件，如果左右两表对应的列列名不同，可以用`left_on`和`right_on`参数取代`on`参数分别进行指定。\n",
    "\n",
    "如果对上面的代码稍作修改，将`how`参数修改为`'right'`，大家可以思考一下代码执行的结果。\n",
    "\n",
    "```Python\n",
    "pd.merge(all_emp_df, dept_df, how='right', on='dno')\n",
    "```\n",
    "\n",
    "运行结果比之前的输出多出了如下所示的一行，这是因为`how='right'`代表右外连接，也就意味着右表`dept_df`中的数据会被完整的查出来，但是在`all_emp_df`中又没有编号为`40` 部门的员工，所以对应的位置都被填入了空值。\n",
    "\n",
    "```\n",
    "19\tNaN    NaN    NaN    NaN    NaN     NaN    40    运维部    深圳\n",
    "```\n",
    "\n",
    "#### 数据清洗\n",
    "\n",
    "通常，我们从 Excel、CSV 或数据库中获取到的数据并不是非常完美的，里面可能因为系统或人为的原因混入了重复值或异常值，也可能在某些字段上存在缺失值；再者，`DataFrame`中的数据也可能存在格式不统一、量纲不统一等各种问题。因此，在开始数据分析之前，对数据进行清洗就显得特别重要。\n",
    "\n",
    "##### 缺失值\n",
    "\n",
    "可以使用`DataFrame`对象的`isnull`或`isna`方法来找出数据表中的缺失值，如下所示。\n",
    "\n",
    "```Python\n",
    "emp_df.isnull()\n",
    "```\n",
    "\n",
    "或者\n",
    "\n",
    "```Python\n",
    "emp_df.isna()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "        ename   job\t    mgr     sal     comm    dno\n",
    "eno\t\t\t\t\t\t\n",
    "1359\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n",
    "2056\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n",
    "3088\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n",
    "3211\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\n",
    "3233\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\n",
    "3244\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\n",
    "3251\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\n",
    "3344\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n",
    "3577\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\n",
    "3588\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\n",
    "4466\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\n",
    "5234\tFalse\tFalse\tFalse\tFalse\tTrue\tFalse\n",
    "5566\tFalse\tFalse\tFalse\tFalse\tFalse\tFalse\n",
    "7800\tFalse\tFalse\tTrue\tFalse\tFalse\tFalse\n",
    "```\n",
    "\n",
    "相对应的，`notnull`和`notna`方法可以将非空的值标记为`True`。如果想删除这些缺失值，可以使用`DataFrame`对象的`dropna`方法，该方法的`axis`参数可以指定沿着0轴还是1轴删除，也就是说当遇到空值时，是删除整行还是删除整列，默认是沿0轴进行删除的，代码如下所示。\n",
    "\n",
    "```Python\n",
    "emp_df.dropna()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "        ename   job      mgr\t sal    comm     dno\n",
    "eno\t\t\t\t\t\t\n",
    "1359\t胡一刀  销售员\t3344.0\t1800   200.0\t30\n",
    "2056\t乔峰    架构师\t 7800.0\t 5000\t1500.0\t 20\n",
    "3088\t李莫愁  设计师\t2056.0\t3500   800.0\t20\n",
    "3344\t黄蓉    销售主管\t7800.0\t3000   800.0\t30\n",
    "5566\t宋远桥  会计师\t7800.0\t4000   1000.0\t10\n",
    "```\n",
    "\n",
    "如果要沿着1轴进行删除，可以使用下面的代码。\n",
    "\n",
    "```Python\n",
    "emp_df.dropna(axis=1)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "        ename    job      sal    dno\n",
    "eno\t\t\t\t\n",
    "1359\t胡一刀   销售员    1800\t30\n",
    "2056\t乔峰     架构师\t  5000\t 20\n",
    "3088\t李莫愁   设计师    3500\t20\n",
    "3211\t张无忌   程序员    3200\t20\n",
    "3233\t丘处机   程序员    3400\t20\n",
    "3244\t欧阳锋   程序员    3200\t20\n",
    "3251\t张翠山   程序员    4000\t20\n",
    "3344\t黄蓉     销售主管  3000\t30\n",
    "3577\t杨过     会计\t   2200\t  10\n",
    "3588\t朱九真   会计\t  2500\t 10\n",
    "4466\t苗人凤   销售员\t 2500   30\n",
    "5234\t郭靖     出纳      2000   10\n",
    "5566\t宋远桥   会计师    4000   10\n",
    "7800\t张三丰   总裁      9000   20\n",
    "```\n",
    "\n",
    "> **注意**：`DataFrame`对象的很多方法都有一个名为`inplace`的参数，该参数的默认值为`False`，表示我们的操作不会修改原来的`DataFrame`对象，而是将处理后的结果通过一个新的`DataFrame`对象返回。如果将该参数的值设置为`True`，那么我们的操作就会在原来的`DataFrame`上面直接修改，方法的返回值为`None`。简单的说，上面的操作并没有修改`emp_df`，而是返回了一个新的`DataFrame`对象。\n",
    "\n",
    "在某些特定的场景下，我们可以对空值进行填充，对应的方法是`fillna`，填充空值时可以使用指定的值（通过`value`参数进行指定），也可以用表格中前一个单元格（通过设置参数`method=ffill`）或后一个单元格（通过设置参数`method=bfill`）的值进行填充，当代码如下所示。\n",
    "\n",
    "```Python\n",
    "emp_df.fillna(value=0)\n",
    "```\n",
    "\n",
    "> **注意**：填充的值如何选择也是一个值得探讨的话题，实际工作中，可能会使用某种统计量（如：均值、众数等）进行填充，或者使用某种插值法（如：随机插值法、拉格朗日插值法等）进行填充，甚至有可能通过回归模型、贝叶斯模型等对缺失数据进行填充。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "        ename    job        mgr      sal     comm    dno\n",
    "eno\n",
    "1359\t胡一刀    销售员\t   3344.0\t1800\t200.0\t30\n",
    "2056\t乔峰\t    分析师\t    7800.0\t 5000\t 1500.0\t 20\n",
    "3088\t李莫愁\t   设计师\t   2056.0\t3500\t800.0\t20\n",
    "3211\t张无忌\t   程序员\t   2056.0\t3200\t0.0     20\n",
    "3233\t丘处机\t   程序员\t   2056.0\t3400\t0.0\t    20\n",
    "3244\t欧阳锋\t   程序员\t   3088.0\t3200\t0.0     20\n",
    "3251\t张翠山\t   程序员\t   2056.0\t4000\t0.0\t    20\n",
    "3344\t黄蓉\t    销售主管   7800.0\t3000\t800.0\t30\n",
    "3577\t杨过\t    会计\t     5566.0\t  2200\t  0.0\t  10\n",
    "3588\t朱九真\t   会计\t    5566.0\t 2500\t 0.0\t 10\n",
    "4466\t苗人凤\t   销售员\t   3344.0\t2500\t0.0\t    30\n",
    "5234\t郭靖\t    出纳\t     5566.0\t  2000\t  0.0\t  10\n",
    "5566\t宋远桥\t   会计师\t   7800.0\t4000\t1000.0\t10\n",
    "7800\t张三丰\t   总裁\t    0.0      9000\t 1200.0\t 20\n",
    "```\n",
    "\n",
    "##### 重复值\n",
    "\n",
    "接下来，我们先给之前的部门表添加两行数据，让部门表中名为“研发部”和“销售部”的部门各有两个。\n",
    "\n",
    "```Python\n",
    "dept_df.loc[50] = {'dname': '研发部', 'dloc': '上海'}\n",
    "dept_df.loc[60] = {'dname': '销售部', 'dloc': '长沙'}\n",
    "dept_df\n",
    "```\n",
    "\n",
    "输出:\n",
    "\n",
    "```\n",
    "    dname  dloc\n",
    "dno\t\t\n",
    "10\t会计部\t北京\n",
    "20\t研发部\t成都\n",
    "30\t销售部\t重庆\n",
    "40\t运维部\t天津\n",
    "50\t研发部\t上海\n",
    "60\t销售部\t长沙\n",
    "```\n",
    "\n",
    "现在，我们的数据表中有重复数据了，我们可以通过`DataFrame`对象的`duplicated`方法判断是否存在重复值，该方法在不指定参数时默认判断行索引是否重复，我们也可以指定根据部门名称`dname`判断部门是否重复，代码如下所示。\n",
    "\n",
    "```Python\n",
    "dept_df.duplicated('dname')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "dno\n",
    "10    False\n",
    "20    False\n",
    "30    False\n",
    "40    False\n",
    "50     True\n",
    "60     True\n",
    "dtype: bool\n",
    "```\n",
    "\n",
    "从上面的输出可以看到，`50`和`60`两个部门从部门名称上来看是重复的，如果要删除重复值，可以使用`drop_duplicates`方法，该方法的`keep`参数可以控制在遇到重复值时，保留第一项还是保留最后一项，或者多个重复项一个都不用保留，全部删除掉。\n",
    "\n",
    "```Python\n",
    "dept_df.drop_duplicates('dname')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "\tdname\tdloc\n",
    "dno\t\t\n",
    "10\t会计部\t北京\n",
    "20\t研发部\t成都\n",
    "30\t销售部\t重庆\n",
    "40\t运维部\t天津\n",
    "```\n",
    "\n",
    "将`keep`参数的值修改为`last`。\n",
    "\n",
    "```Python\n",
    "dept_df.drop_duplicates('dname', keep='last')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "\tdname\tdloc\n",
    "dno\t\t\n",
    "10\t会计部\t北京\n",
    "40\t运维部\t天津\n",
    "50\t研发部\t上海\n",
    "60\t销售部\t长沙\n",
    "```\n",
    "\n",
    "使用同样的方式，我们也可以清除`all_emp_df`中的重复数据，例如我们认定“ename”和“job”两个字段完全相同的就是重复数据，我们可以用下面的代码去除重复数据。\n",
    "\n",
    "```python\n",
    "all_emp_df.drop_duplicates(['ename', 'job'], inplace=True)\n",
    "```\n",
    "\n",
    "> **说明**：上面的`drop_duplicates`方法添加了参数`inplace=True`，该方法不会返回新的`DataFrame`对象，而是在原来的`DataFrame`对象上直接删除，大家可以查看`all_emp_df`看看是不是已经移除了重复的员工数据。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### 异常值\n",
    "\n",
    "异常值在统计学上的全称是疑似异常值，也称作离群点（outlier），异常值的分析也称作离群点分析。异常值是指样本中出现的“极端值”，数据值看起来异常大或异常小，其分布明显偏离其余的观测值。实际工作中，有些异常值可能是由系统或人为原因造成的，但有些异常值却不是，它们能够重复且稳定的出现，属于正常的极端值，例如很多游戏产品中头部玩家的数据往往都是离群的极端值。所以，我们既不能忽视异常值的存在，也不能简单地把异常值从数据分析中剔除。重视异常值的出现，分析其产生的原因，常常成为发现问题进而改进决策的契机。\n",
    "\n",
    "异常值的检测有Z-score 方法、IQR 方法、DBScan 聚类、孤立森林等，这里我们对前两种方法做一个简单的介绍。\n",
    "\n",
    "<img src=\"http://localhost/mypic/20211004192858.png\" style=\"zoom:50%;\">\n",
    "\n",
    "如果数据服从正态分布，依据3σ法则，异常值被定义与平均值的偏差超过三倍标准差的值。在正态分布下，距离平均值3σ之外的值出现的概率为$ P(|x-\\mu|>3\\sigma)<0.003 $，属于小概率事件。如果数据不服从正态分布，那么可以用远离均值的多少倍的标准差来描述，这里的倍数就是Z-score。Z-score以标准差为单位去度量某一原始分数偏离平均值的距离，公式如下所示。\n",
    "$$\n",
    "z = \\frac {X - \\mu} {\\sigma} \\\\\n",
    "|z| > 3\n",
    "$$\n",
    "Z-score需要根据经验和实际情况来决定，通常把远离标准差`3`倍距离以上的数据点视为离群点，下面的代给出了如何通过Z-score方法检测异常值。\n",
    "\n",
    "```Python\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    avg_value = np.mean(data)\n",
    "    std_value = np.std(data)\n",
    "    z_score = np.abs((data - avg_value) / std_value)\n",
    "    return data[z_score > threshold]\n",
    "```\n",
    "\n",
    "IQR 方法中的IQR（Inter-Quartile Range）代表四分位距离，即上四分位数（Q3）和下四分位数（Q1）的差值。通常情况下，可以认为小于 $ Q1 - 1.5 \\times IQR $ 或大于 $ Q3 + 1.5 \\times IQR $ 的就是异常值，而这种检测异常值的方法也是箱线图（后面会讲到）默认使用的方法。下面的代码给出了如何通过 IQR 方法检测异常值。\n",
    "\n",
    "```Python\n",
    "def detect_outliers_iqr(data, whis=1.5):\n",
    "    q1, q3 = np.quantile(data, [0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - whis * iqr, q3 + whis * iqr\n",
    "    return data[(data < lower) | (data > upper)]\n",
    "```\n",
    "\n",
    "如果要删除异常值，可以使用`DataFrame`对象的`drop`方法，该方法可以根据行索引或列索引删除指定的行或列。例如我们认为月薪低于`2000`或高于`8000`的是员工表中的异常值，可以用下面的代码删除对应的记录。\n",
    "\n",
    "```Python\n",
    "emp_df.drop(emp_df[(emp_df.sal > 8000) | (emp_df.sal < 2000)].index)\n",
    "```\n",
    "\n",
    "如果要替换掉异常值，可以通过给单元格赋值的方式来实现，也可以使用`replace`方法将指定的值替换掉。例如我们要将月薪为`1800`和`9000`的替换为月薪的平均值，补贴为`800`的替换为`1000`，代码如下所示。\n",
    "\n",
    "```Python\n",
    "avg_sal = np.mean(emp_df.sal).astype(int)\n",
    "emp_df.replace({'sal': [1800, 9000], 'comm': 800}, {'sal': avg_sal, 'comm': 1000})\n",
    "```\n",
    "\n",
    "#### 预处理\n",
    "\n",
    "对数据进行预处理也是一个很大的话题，它包含了对数据的拆解、变换、归约、离散化等操作。我们先来看看数据的拆解。如果数据表中的数据是一个时间日期，我们通常都需要从年、季度、月、日、星期、小时、分钟等维度对其进行拆解，如果时间日期是用字符串表示的，可以先通过`pandas`的`to_datetime`函数将其处理成时间日期。\n",
    "\n",
    "在下面的例子中，我们先读取 Excel 文件，获取到一组销售数据，其中第一列就是销售日期，我们将其拆解为“月份”、“季度”和“星期”，代码如下所示。\n",
    "\n",
    "```Python\n",
    "sales_df = pd.read_excel(\n",
    "    'data/2020年销售数据.xlsx',\n",
    "    usecols=['销售日期', '销售区域', '销售渠道', '品牌', '售价']\n",
    ")\n",
    "sales_df.info()\n",
    "```\n",
    "\n",
    "> **说明**：上面代码中使用了相对路径来获取 Excel 文件，也就是说 Excel 文件在当前工作路径下名为`data`的文件夹中。如果需要上面例子中的 Excel 文件，可以通过下面的百度云盘地址进行获取。链接：<https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g>，提取码：e7b4。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1945 entries, 0 to 1944\n",
    "Data columns (total 5 columns):\n",
    " #   Column  Non-Null Count  Dtype         \n",
    "---  ------  --------------  -----         \n",
    " 0   销售日期    1945 non-null   datetime64[ns]\n",
    " 1   销售区域    1945 non-null   object        \n",
    " 2   销售渠道    1945 non-null   object        \n",
    " 3   品牌        1945 non-null   object        \n",
    " 4   销售额      1945 non-null   int64         \n",
    "dtypes: datetime64[ns](1), int64(1), object(3)\n",
    "memory usage: 76.1+ KB\n",
    "```\n",
    "\n",
    "```Python\n",
    "sales_df['月份'] = sales_df['销售日期'].dt.month\n",
    "sales_df['季度'] = sales_df['销售日期'].dt.quarter\n",
    "sales_df['星期'] = sales_df['销售日期'].dt.weekday\n",
    "sales_df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "\t    销售日期\t 销售区域\t销售渠道\t品牌\t  销售额\t月份\t季度\t星期\n",
    "0\t    2020-01-01\t上海\t     拼多多\t 八匹马   8217\t    1\t 1\t   2\n",
    "1\t    2020-01-01\t上海\t     抖音\t      八匹马\t6351\t 1\t  1\t    2\n",
    "2\t    2020-01-01\t上海\t     天猫\t      八匹马\t14365\t 1\t  1\t    2\n",
    "3\t    2020-01-01\t上海\t     天猫       八匹马\t2366\t 1\t  1     2\n",
    "4\t    2020-01-01\t上海\t     天猫 \t  皮皮虾\t15189\t 1\t  1     2\n",
    "...     ...         ...        ...       ...      ...     ...  ...   ...\n",
    "1940    2020-12-30\t北京\t     京东\t      花花姑娘 6994     12\t 4\t   2\n",
    "1941    2020-12-30\t福建\t     实体\t      八匹马\t7663\t 12\t  4\t    2\n",
    "1942    2020-12-31\t福建\t     实体\t      花花姑娘 14795    12\t 4\t   3\n",
    "1943    2020-12-31\t福建\t     抖音\t      八匹马\t3481\t 12\t  4\t    3\n",
    "1944    2020-12-31\t福建\t     天猫\t      八匹马\t2673\t 12\t  4\t    3\n",
    "```\n",
    "\n",
    "在上面的代码中，通过日期时间类型的`Series`对象的`dt` 属性，获得一个访问日期时间的对象，通过该对象的`year`、`month`、`quarter`、`hour`等属性，就可以获取到年、月、季度、小时等时间信息，获取到的仍然是一个`Series`对象，它包含了一组时间信息，所以我们通常也将这个`dt`属性称为“日期时间向量”。\n",
    "\n",
    "我们再来说一说字符串类型的数据的处理，我们先从指定的 Excel 文件中读取某招聘网站的招聘数据。\n",
    "\n",
    "```Python\n",
    "jobs_df = pd.read_csv(\n",
    "    'data/某招聘网站招聘数据.csv',\n",
    "    usecols=['city', 'companyFullName', 'positionName', 'salary']\n",
    ")\n",
    "jobs_df.info()\n",
    "```\n",
    "\n",
    "> **说明**：上面代码中使用了相对路径来获取 CSV 文件，也就是说 CSV 文件在当前工作路径下名为`data`的文件夹中。如果需要上面例子中的 CSV 文件，可以通过下面的百度云盘地址进行获取。链接：<https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g>，提取码：e7b4。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 3140 entries, 0 to 3139\n",
    "Data columns (total 4 columns):\n",
    " #   Column           Non-Null Count  Dtype \n",
    "---  ------           --------------  ----- \n",
    " 0   city             3140 non-null   object\n",
    " 1   companyFullName  3140 non-null   object\n",
    " 2   positionName     3140 non-null   object\n",
    " 3   salary           3140 non-null   object\n",
    "dtypes: object(4)\n",
    "memory usage: 98.2+ KB\n",
    "```\n",
    "\n",
    "查看前`5`条数据。\n",
    "\n",
    "```Python\n",
    "jobs_df.head()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "    city    companyFullName              positionName    salary\n",
    "0   北京\t  达疆网络科技（上海）有限公司    数据分析岗       15k-30k\n",
    "1   北京\t  北京音娱时光科技有限公司        数据分析        10k-18k\n",
    "2   北京\t  北京千喜鹤餐饮管理有限公司\t     数据分析        20k-30k\n",
    "3   北京\t  吉林省海生电子商务有限公司\t     数据分析        33k-50k\n",
    "4   北京\t  韦博网讯科技（北京）有限公司\t数据分析        10k-15k\n",
    "```\n",
    "\n",
    "上面的数据表一共有`3140`条数据，但并非所有的职位都是“数据分析”的岗位，如果要筛选出数据分析的岗位，可以通过检查`positionName`字段是否包含“数据分析”这个关键词，这里需要模糊匹配，应该如何实现呢？我们可以先获取`positionName`列，因为这个`Series`对象的`dtype`是字符串，所以可以通过`str`属性获取对应的字符串向量，然后就可以利用我们熟悉的字符串的方法来对其进行操作，代码如下所示。\n",
    "\n",
    "```Python\n",
    "jobs_df = jobs_df[jobs_df.positionName.str.contains('数据分析')]\n",
    "jobs_df.shape\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "(1515, 4)\n",
    "```\n",
    "\n",
    "可以看出，筛选后的数据还有`1515`条。接下来，我们还需要对`salary`字段进行处理，如果我们希望统计所有岗位的平均工资或每个城市的平均工资，首先需要将用范围表示的工资处理成其中间值，代码如下所示。\n",
    "\n",
    "```Python\n",
    "jobs_df.salary.str.extract(r'(\\d+)[kK]?-(\\d+)[kK]?')\n",
    "```\n",
    "\n",
    "> **说明**：上面的代码通过正则表达式捕获组从字符串中抽取出两组数字，分别对应工资的下限和上限，对正则表达式不熟悉的读者，可以阅读我的知乎专栏“从零开始学Python”中的[《正则表达式的应用》](https://zhuanlan.zhihu.com/p/158929767)一文。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "        0     1\n",
    "0\t    15    30\n",
    "1\t    10\t  18\n",
    "2       20    30\n",
    "3       33    50\n",
    "4       10    15\n",
    "...     ...   ...\n",
    "3065    8     10\n",
    "3069    6     10\n",
    "3070    2     4\n",
    "3071    6     12\n",
    "3088    8     12\n",
    "```\n",
    "\n",
    "需要提醒大家的是，抽取出来的两列数据都是字符串类型的值，我们需要将其转换成`int`类型，才能计算平均值，对应的方法是`DataFrame`对象的`applymap`方法，该方法的参数是一个函数，而该函数会作用于`DataFrame`中的每个元素。完成这一步之后，我们就可以使用`apply`方法将上面的`DataFrame`处理成中间值，`apply`方法的参数也是一个函数，可以通过指定`axis`参数使其作用于`DataFrame` 对象的行或列，代码如下所示。\n",
    "\n",
    "```Python\n",
    "temp_df = jobs_df.salary.str.extract(r'(\\d+)[kK]?-(\\d+)[kK]?').applymap(int)\n",
    "temp_df.apply(np.mean, axis=1)\n",
    "```\n",
    "\n",
    " 输出：\n",
    "\n",
    "```\n",
    "0       22.5\n",
    "1       14.0\n",
    "2       25.0\n",
    "3       41.5\n",
    "4       12.5\n",
    "        ... \n",
    "3065    9.0\n",
    "3069    8.0\n",
    "3070    3.0\n",
    "3071    9.0\n",
    "3088    10.0\n",
    "Length: 1515, dtype: float64\n",
    "```\n",
    "\n",
    "接下来，我们可以用上面的结果替换掉原来的`salary`列或者增加一个新的列来表示职位对应的工资，完整的代码如下所示。\n",
    "\n",
    "```Python\n",
    "temp_df = jobs_df.salary.str.extract(r'(\\d+)[kK]?-(\\d+)[kK]?').applymap(int)\n",
    "jobs_df['salary'] = temp_df.apply(np.mean, axis=1)\n",
    "jobs_df.head()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "    city    companyFullName              positionName    salary\n",
    "0   北京\t  达疆网络科技（上海）有限公司    数据分析岗       22.5\n",
    "1   北京\t  北京音娱时光科技有限公司        数据分析        14.0\n",
    "2   北京\t  北京千喜鹤餐饮管理有限公司\t     数据分析        25.0\n",
    "3   北京\t  吉林省海生电子商务有限公司\t     数据分析        41.5\n",
    "4   北京\t  韦博网讯科技（北京）有限公司\t数据分析        12.5\n",
    "```\n",
    "\n",
    "`applymap`和`apply`两个方法在数据预处理的时候经常用到，`Series`对象也有`apply`方法，也是用于数据的预处理，但是`DataFrame`对象还有一个名为`transform` 的方法，也是通过传入的函数对数据进行变换，类似`Series`对象的`map`方法。需要强调的是，`apply`方法具有归约效果的，简单的说就是能将较多的数据处理成较少的数据或一条数据；而`transform`方法没有归约效果，只能对数据进行变换，原来有多少条数据，处理后还是有多少条数据。\n",
    "\n",
    "如果要对数据进行深度的分析和挖掘，字符串、日期时间这样的非数值类型都需要处理成数值，因为非数值类型没有办法计算相关性，也没有办法进行$\\chi^2$检验等操作。对于字符串类型，通常可以其分为以下三类，再进行对应的处理。\n",
    "\n",
    "1. 有序变量（Ordinal Variable）：字符串表示的数据有顺序关系，那么可以对字符串进行序号化处理。\n",
    "2. 分类变量（Categorical Variable）/ 名义变量（Nominal Variable）：字符串表示的数据没有大小关系和等级之分，那么就可以使用独热编码的方式处理成哑变量（虚拟变量）矩阵。\n",
    "3. 定距变量（Scale Variable）：字符串本质上对应到一个有大小高低之分的数据，而且可以进行加减运算，那么只需要将字符串处理成对应的数值即可。\n",
    "\n",
    "对于第1类和第3类，我们可以用上面提到的`apply`或`transform`方法来处理，也可以利用`scikit-learn`中的`OrdinalEncoder`处理第1类字符串，这个我们在后续的课程中会讲到。对于第2类字符串，可以使用`pandas`的`get_dummies()`函数来生成哑变量（虚拟变量）矩阵，代码如下所示。\n",
    "\n",
    "```Python\n",
    "persons_df = pd.DataFrame(\n",
    "    data={\n",
    "        '姓名': ['关羽', '张飞', '赵云', '马超', '黄忠'],\n",
    "        '职业': ['医生', '医生', '程序员', '画家', '教师'],\n",
    "        '学历': ['研究生', '大专', '研究生', '高中', '本科']\n",
    "    }\n",
    ")\n",
    "persons_df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "\t姓名\t职业\t学历\n",
    "0\t关羽\t医生\t研究生\n",
    "1\t张飞\t医生\t大专\n",
    "2\t赵云\t程序员\t研究生\n",
    "3\t马超\t画家\t高中\n",
    "4\t黄忠\t教师\t本科\n",
    "```\n",
    "\n",
    "将职业处理成哑变量矩阵。\n",
    "\n",
    "```Python\n",
    "pd.get_dummies(persons_df['职业'])\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "    医生 教师  画家  程序员\n",
    "0\t1    0    0    0\n",
    "1\t1    0    0    0\n",
    "2\t0    0    0    1\n",
    "3\t0    0    1    0\n",
    "4\t0    1    0    0\n",
    "```\n",
    "\n",
    "将学历处理成大小不同的值。\n",
    "\n",
    "```Python\n",
    "def handle_education(x):\n",
    "    edu_dict = {'高中': 1, '大专': 3, '本科': 5, '研究生': 10}\n",
    "    return edu_dict.get(x, 0)\n",
    "\n",
    "\n",
    "persons_df['学历'].apply(handle_education)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "0    10\n",
    "1     3\n",
    "2    10\n",
    "3     1\n",
    "4     5\n",
    "Name: 学历, dtype: int64\n",
    "```\n",
    "\n",
    "我们再来说说数据离散化。离散化也叫分箱，如果变量的取值是连续值，那么它的取值有无数种可能，在进行数据分组的时候就会非常的不方便，这个时候将连续变量离散化就显得非常重要。之所以把离散化叫做分箱，是因为我们可以预先设置一些箱子，每个箱子代表了数据取值的范围，这样就可以将连续的值分配到不同的箱子中，从而实现离散化。下面的例子读取了2018年北京积分落户数据，我们可以根据落户积分对数据进行分组，具体的做法如下所示。\n",
    "\n",
    "```Python\n",
    "luohu_df = pd.read_csv('data/2018年北京积分落户数据.csv', index_col='id')\n",
    "luohu_df.score.describe()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "count    6019.000000\n",
    "mean       95.654552\n",
    "std         4.354445\n",
    "min        90.750000\n",
    "25%        92.330000\n",
    "50%        94.460000\n",
    "75%        97.750000\n",
    "max       122.590000\n",
    "Name: score, dtype: float64\n",
    "```\n",
    "\n",
    "可以看出，落户积分的最大值是`122.59`，最小值是`90.75`，那么我们可以构造一个从`90`分到`125`分，每`5`分一组的`7`个箱子，`pandas`的`cut`函数可以帮助我们首先数据分箱，代码如下所示。\n",
    "\n",
    "```Python\n",
    "bins = np.arange(90, 126, 5)\n",
    "pd.cut(luohu_df.score, bins, right=False)\n",
    "```\n",
    "\n",
    "> **说明**：`cut`函数的`right`参数默认值为`True`，表示箱子左开右闭；修改为`False`可以让箱子的右边界为开区间，左边界为闭区间，大家看看下面的输出就明白了。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "id\n",
    "1       [120, 125)\n",
    "2       [120, 125)\n",
    "3       [115, 120)\n",
    "4       [115, 120)\n",
    "5       [115, 120)\n",
    "           ...    \n",
    "6015      [90, 95)\n",
    "6016      [90, 95)\n",
    "6017      [90, 95)\n",
    "6018      [90, 95)\n",
    "6019      [90, 95)\n",
    "Name: score, Length: 6019, dtype: category\n",
    "Categories (7, interval[int64, left]): [[90, 95) < [95, 100) < [100, 105) < [105, 110) < [110, 115) < [115, 120) < [120, 125)]\n",
    "```\n",
    "\n",
    "我们可以根据分箱的结果对数据进行分组，然后使用聚合函数对每个组进行统计，这是数据分析中经常用到的操作，下一个章节会为大家介绍。除此之外，`pandas`还提供了一个名为`qcut`的函数，可以指定分位数对数据进行分箱，有兴趣的读者可以自行研究。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcdfd2-de31-41b3-b59e-8cae51e1119a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad68aba-e13d-4cc4-90b7-1774ba0d7585",
   "metadata": {},
   "source": [
    "### 数据透视\n",
    "\n",
    "经过前面的学习，我们已经将数据准备就绪而且变成了我们想要的样子，接下来就是最为重要的数据透视阶段了。当我们拿到一大堆数据的时候，如何从数据中迅速的解读出有价值的信息，把繁杂的数据变成容易解读的统计图表并再此基础上产生业务洞察，这就是数据分析要解决的核心问题。\n",
    "\n",
    "#### 获取描述性统计信息\n",
    "\n",
    "首先，我们可以获取数据的描述性统计信息，通过描述性统计信息，我们可以了解数据的集中趋势和离散趋势。\n",
    "\n",
    "例如，我们有如下所示的学生成绩表。\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "scores = np.random.randint(50, 101, (5, 3))\n",
    "names = ('关羽', '张飞', '赵云', '马超', '黄忠')\n",
    "courses = ('语文', '数学', '英语')\n",
    "df = pd.DataFrame(data=scores, columns=courses, index=names)\n",
    "df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "     语文   数学   英语\n",
    "关羽  96    72    73\n",
    "张飞  72    70\t97\n",
    "赵云  74    51\t79\n",
    "马超  100   54\t54\n",
    "黄忠  89    100\t88\n",
    "```\n",
    "\n",
    "我们可以通过`DataFrame`对象的方法`mean`、`max`、`min`、`std`、`var`等方法分别获取每个学生或每门课程的平均分、最高分、最低分、标准差、方差等信息，也可以直接通过`describe`方法直接获取描述性统计信息，代码如下所示。\n",
    "\n",
    "计算每门课程成绩的平均分。\n",
    "\n",
    "```Python\n",
    "df.mean()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "语文    86.2\n",
    "数学    69.4\n",
    "英语    78.2\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "计算每个学生成绩的平均分。\n",
    "\n",
    "```Python\n",
    "df.mean(axis=1)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "关羽    80.333333\n",
    "张飞    79.666667\n",
    "赵云    68.000000\n",
    "马超    69.333333\n",
    "黄忠    92.333333\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "计算每门课程成绩的方差。\n",
    "\n",
    "```Python\n",
    "df.var()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "语文    161.2\n",
    "数学    379.8\n",
    "英语    265.7\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "> **说明**：通过方差可以看出，数学成绩波动最大，两极分化可能更严重。\n",
    "\n",
    "获取每门课程的描述性统计信息。\n",
    "\n",
    "```Python\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "        语文        数学         英语\n",
    "count   5.000000\t5.000000\t5.000000\n",
    "mean    86.200000\t69.400000\t78.200000\n",
    "std     12.696456\t19.488458\t16.300307\n",
    "min     72.000000\t51.000000\t54.000000\n",
    "25%     74.000000\t54.000000\t73.000000\n",
    "50%     89.000000\t70.000000\t79.000000\n",
    "75%     96.000000\t72.000000\t88.000000\n",
    "max     100.000000\t100.000000\t97.000000\n",
    "```\n",
    "\n",
    "#### 排序和取头部值\n",
    "\n",
    "如果需要对数据进行排序，可以使用`DataFrame`对象的`sort_values`方法，该方法的`by`参数可以指定根据哪个列或哪些列进行排序，而`ascending`参数可以指定升序或是降序。例如，下面的代码展示了如何将学生表按语文成绩排降序。\n",
    "\n",
    "```Python\n",
    "df.sort_values(by='语文', ascending=False)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "      语文   数学   英语\n",
    "马超\t100    54\t  54\n",
    "关羽\t96     72     73\n",
    "黄忠\t89     100    88\n",
    "赵云\t74     51     79\n",
    "张飞\t72     70     97\n",
    "```\n",
    "\n",
    "如果`DataFrame`数据量很大，排序将是一个非常耗费时间的操作。有的时候我们只需要获得排前N名或后N名的数据，这个时候其实没有必要对整个数据进行排序，而是直接利用堆结构找出Top-N的数据。`DataFrame`的`nlargest`和`nsmallest`方法就提供对Top-N操作的支持，代码如下所示。\n",
    "\n",
    "找出语文成绩前3名的学生信息。\n",
    "\n",
    "```Python\n",
    "df.nlargest(3, '语文')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "      语文   数学   英语\n",
    "马超\t100    54\t  54\n",
    "关羽\t96     72     73\n",
    "黄忠\t89     100    88\n",
    "```\n",
    "\n",
    "找出数学成绩最低的3名学生的信息。\n",
    "\n",
    "```Python\n",
    "df.nsmallest(3, '数学')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "      语文  数学  英语\n",
    "赵云  74    51\t79\n",
    "马超  100   54\t54\n",
    "张飞  72    70\t97\n",
    "```\n",
    "\n",
    "#### 分组聚合\n",
    "\n",
    "我们先从之前使用过的 Excel 文件中读取2020年销售数据，然后再为大家演示如何进行分组聚合操作。\n",
    "\n",
    "```Python\n",
    "df = pd.read_excel('data/2020年销售数据.xlsx')\n",
    "df.head()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "    销售日期\t 销售区域   销售渠道  销售订单     品牌    售价  销售数量\n",
    "0   2020-01-01  上海       拼多多    182894-455  八匹马  99    83\n",
    "1   2020-01-01  上海       抖音      205635-402  八匹马  219   29\n",
    "2   2020-01-01  上海       天猫      205654-021  八匹马  169   85\n",
    "3   2020-01-01  上海       天猫      205654-519  八匹马  169   14\n",
    "4   2020-01-01  上海       天猫      377781-010  皮皮虾  249   61\n",
    "```\n",
    "\n",
    "如果我们要统计每个销售区域的销售总额，可以先通过“售价”和“销售数量”计算出销售额，为`DataFrame`添加一个列，代码如下所示。\n",
    "\n",
    "```Python\n",
    "df['销售额'] = df['售价'] * df['销售数量']\n",
    "df.head()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "    销售日期\t 销售区域   销售渠道  销售订单     品牌    售价  销售数量  销售额\n",
    "0   2020-01-01  上海       拼多多    182894-455  八匹马  99    83        8217\n",
    "1   2020-01-01  上海       抖音      205635-402  八匹马  219   29        6351\n",
    "2   2020-01-01  上海       天猫      205654-021  八匹马  169   85        14365\n",
    "3   2020-01-01  上海       天猫      205654-519  八匹马  169   14        2366\n",
    "4   2020-01-01  上海       天猫      377781-010  皮皮虾  249   61        15189\n",
    "```\n",
    "\n",
    "然后再根据“销售区域”列对数据进行分组，这里我们使用的是`DataFrame`对象的`groupby`方法。分组之后，我们取“销售额”这个列在分组内进行求和处理，代码和结果如下所示。\n",
    "\n",
    "```Python\n",
    "df.groupby('销售区域').销售额.sum()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "销售区域\n",
    "上海    11610489\n",
    "北京    12477717\n",
    "安徽      895463\n",
    "广东     1617949\n",
    "江苏     2304380\n",
    "浙江      687862\n",
    "福建    10178227\n",
    "Name: 销售额, dtype: int64\n",
    "```\n",
    "\n",
    "如果我们要统计每个月的销售总额，我们可以将“销售日期”作为groupby`方法的参数，当然这里需要先将“销售日期”处理成月，代码和结果如下所示。\n",
    "\n",
    "```Python\n",
    "df.groupby(df['销售日期'].dt.month).销售额.sum()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "销售日期\n",
    "1     5409855\n",
    "2     4608455\n",
    "3     4164972\n",
    "4     3996770\n",
    "5     3239005\n",
    "6     2817936\n",
    "7     3501304\n",
    "8     2948189\n",
    "9     2632960\n",
    "10    2375385\n",
    "11    2385283\n",
    "12    1691973\n",
    "Name: 销售额, dtype: int64\n",
    "```\n",
    "\n",
    "接下来我们将难度升级，统计每个销售区域每个月的销售总额，这又该如何处理呢？事实上，`groupby`方法的第一个参数可以是一个列表，列表中可以指定多个分组的依据，大家看看下面的代码和输出结果就明白了。\n",
    "\n",
    "```Python\n",
    "df.groupby(['销售区域', df['销售日期'].dt.month]).销售额.sum()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "销售区域  销售日期\n",
    "上海    1       1679125\n",
    "        2       1689527\n",
    "        3       1061193\n",
    "        4       1082187\n",
    "        5        841199\n",
    "        6        785404\n",
    "        7        863906\n",
    "        8        734937\n",
    "        9       1107693\n",
    "        10       412108\n",
    "       11       825169\n",
    "       12       528041\n",
    "北京    1       1878234\n",
    "        2       1807787\n",
    "        3       1360666\n",
    "        4       1205989\n",
    "        5        807300\n",
    "        6       1216432\n",
    "        7       1219083\n",
    "        8        645727\n",
    "        9        390077\n",
    "        10       671608\n",
    "        11       678668\n",
    "        12       596146\n",
    "安徽    4        341308\n",
    "        5        554155\n",
    "广东    3        388180\n",
    "        8        469390\n",
    "        9        365191\n",
    "        11       395188\n",
    "江苏    4        537079\n",
    "        7        841032\n",
    "        10       710962\n",
    "        12       215307\n",
    "浙江    3        248354\n",
    "        8        439508\n",
    "福建    1       1852496\n",
    "        2       1111141\n",
    "        3       1106579\n",
    "        4        830207\n",
    "        5       1036351\n",
    "        6        816100\n",
    "        7        577283\n",
    "        8        658627\n",
    "        9        769999\n",
    "        10       580707\n",
    "        11       486258\n",
    "        12       352479\n",
    "Name: 销售额, dtype: int64\n",
    "```\n",
    "\n",
    "如果希望统计出每个区域的销售总额以及每个区域单笔金额的最高和最低，我们可以在`DataFrame`或`Series`对象上使用`agg`方法并指定多个聚合函数，代码和结果如下所示。\n",
    "\n",
    "```Python\n",
    "df.groupby('销售区域').销售额.agg(['sum', 'max', 'min'])\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "           sum     max   min\n",
    "销售区域                        \n",
    "上海    11610489  116303   948\n",
    "北京    12477717  133411   690\n",
    "安徽      895463   68502  1683\n",
    "广东     1617949  120807   990\n",
    "江苏     2304380  114312  1089\n",
    "浙江      687862   90909  3927\n",
    "福建    10178227   87527   897\n",
    "```\n",
    "\n",
    "如果希望自定义聚合后的列的名字，可以使用如下所示的方法。\n",
    "\n",
    "```Python\n",
    "df.groupby('销售区域').销售额.agg(销售总额='sum', 单笔最高='max', 单笔最低='min')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "          销售总额    单笔最高  单笔最低\n",
    "销售区域                        \n",
    "上海      11610489     116303     948\n",
    "北京      12477717     133411     690\n",
    "安徽        895463      68502    1683\n",
    "广东       1617949     120807     990\n",
    "江苏       2304380     114312    1089\n",
    "浙江        687862      90909    3927\n",
    "福建      10178227      87527     897\n",
    "```\n",
    "\n",
    "如果需要对多个列使用不同的聚合函数，例如“统计每个销售区域销售额的总和以及销售数量的最低值和最高值”，我们可以按照下面的方式来操作。\n",
    "\n",
    "```Python\n",
    "df.groupby('销售区域')[['销售额', '销售数量']].agg({\n",
    "    '销售额': 'sum', '销售数量': ['max', 'min']\n",
    "})\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "           销售额  销售数量    \n",
    "           sum    max min\n",
    "销售区域                   \n",
    "上海    11610489  100  10\n",
    "北京    12477717  100  10\n",
    "安徽      895463   98  16\n",
    "广东     1617949   98  10\n",
    "江苏     2304380  100  11\n",
    "浙江      687862   95  20\n",
    "福建    10178227  100  10\n",
    "```\n",
    "\n",
    "#### 透视表和交叉表\n",
    "\n",
    "上面的例子中，“统计每个销售区域每个月的销售总额”会产生一个看起来很长的结果，在实际工作中我们通常把那些行很多列很少的表成为“窄表”，如果我们不想得到这样的一个“窄表”，可以使用`DataFrame`的`pivot_table`方法或者是`pivot_table`函数来生成透视表。透视表的本质就是对数据进行分组聚合操作，**根据 A 列对 B 列进行统计**，如果大家有使用 Excel 的经验，相信对透视表这个概念一定不会陌生。例如，我们要“统计每个销售区域的销售总额”，那么“销售区域”就是我们的 A 列，而“销售额”就是我们的 B 列，在`pivot_table`函数中分别对应`index`和`values`参数，这两个参数都可以是单个列或者多个列。\n",
    "\n",
    "```Python\n",
    "pd.pivot_table(df, index='销售区域', values='销售额', aggfunc='sum')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "           销售额\n",
    "销售区域          \n",
    "上海    11610489\n",
    "北京    12477717\n",
    "安徽      895463\n",
    "广东     1617949\n",
    "江苏     2304380\n",
    "浙江      687862\n",
    "福建    10178227\n",
    "```\n",
    "\n",
    "> **注意**：上面的结果操作跟之前用`groupby`的方式得到的结果有一些区别，`groupby`操作后，如果对单个列进行聚合，得到的结果是一个`Series`对象，而上面的结果是一个`DataFrame` 对象。\n",
    "\n",
    "如果要统计每个销售区域每个月的销售总额，也可以使用`pivot_table`函数，代码如下所示。\n",
    "\n",
    "```Python\n",
    "df['月份'] = df['销售日期'].dt.month\n",
    "pd.pivot_table(df, index=['销售区域', '月份'], values='销售额', aggfunc='sum')\n",
    "```\n",
    "\n",
    "上面的操作结果是一个`DataFrame`，但也是一个长长的“窄表”，如果希望做成一个行比较少列比较多的“宽表”，可以将`index`参数中的列放到`columns`参数中，代码如下所示。\n",
    "\n",
    "```Python\n",
    "pd.pivot_table(df, index='销售区域', columns='月份', values='销售额', aggfunc='sum', fill_value=0)\n",
    "```\n",
    "\n",
    "> **说明**：`pivot_table`函数的`fill_value=0`会将空值处理为`0`。\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/pivot_table_1.png\" style=\"zoom:50%;\">\n",
    "\n",
    "使用`pivot_table`函数时，还可以通过添加`margins`和`margins_name`参数对分组聚合的结果做一个汇总，具体的操作和效果如下所示。\n",
    "\n",
    "```Python\n",
    "pd.pivot_table(df, index='销售区域', columns='月份', values='销售额', aggfunc='sum', fill_value=0, margins=True, margins_name='总计')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/pivot_table_2.png\" style=\"zoom:50%;\">\n",
    "\n",
    "交叉表就是一种特殊的透视表，它不需要先构造一个`DataFrame`对象，而是直接通过数组或`Series`对象指定两个或多个因素进行运算得到统计结果。例如，我们要统计每个销售区域的销售总额，也可以按照如下所示的方式来完成，我们先准备三组数据。\n",
    "\n",
    "```Python\n",
    "sales_area, sales_month, sales_amount = df['销售区域'], df['月份'], df['销售额']\n",
    "```\n",
    "\n",
    "使用`crosstab`函数生成交叉表。\n",
    "\n",
    "```Python\n",
    "pd.crosstab(index=sales_area, columns=sales_month, values=sales_amount, aggfunc='sum').fillna(0).astype('i8')\n",
    "```\n",
    "\n",
    "> **说明**：上面的代码使用了`DataFrame`对象的`fillna`方法将空值处理为0，再使用`astype`方法将数据类型处理成整数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bdf343-81a8-42fe-92a6-08460246682b",
   "metadata": {},
   "source": [
    "#### 计算同比环比\n",
    "\n",
    "我们之前讲过一个统计月度销售额的例子，我们可以通过`groupby`方法做分组聚合，也可以通过`pivot_table`生成透视表，如下所示。\n",
    "\n",
    "```python\n",
    "sales_df = pd.read_excel('data/2020年销售数据.xlsx')\n",
    "sales_df['月份'] = sales_df.销售日期.dt.month\n",
    "sales_df['销售额'] = sales_df.售价 * sales_df.销售数量\n",
    "result_df = sales_df.pivot_table(index='月份', values='销售额', aggfunc='sum')\n",
    "result_df.rename(columns={'销售额': '本月销售额'}, inplace=True)\n",
    "result_df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "      本月销售额\n",
    "月份         \n",
    "1       5409855\n",
    "2       4608455\n",
    "3       4164972\n",
    "4       3996770\n",
    "5       3239005\n",
    "6       2817936\n",
    "7       3501304\n",
    "8       2948189\n",
    "9       2632960\n",
    "10      2375385\n",
    "11      2385283\n",
    "12      1691973\n",
    "```\n",
    "\n",
    "在得到月度销售额之后，如果我们需要计算月环比，这里有两种方案。第一种方案是我们可以使用`shift`方法对数据进行移动，将上一个月的数据与本月数据对齐，然后通过`(本月销售额 - 上月销售额) / 上月销售额`来计算月环比，代码如下所示。\n",
    "\n",
    "```python\n",
    "result_df['上月销售额'] = result_df.本月销售额.shift(1)\n",
    "result_df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "      本月销售额      上月销售额\n",
    "月份                    \n",
    "1       5409855            NaN\n",
    "2       4608455      5409855.0\n",
    "3       4164972      4608455.0\n",
    "4       3996770      4164972.0\n",
    "5       3239005      3996770.0\n",
    "6       2817936      3239005.0\n",
    "7       3501304      2817936.0\n",
    "8       2948189      3501304.0\n",
    "9       2632960      2948189.0\n",
    "10      2375385      2632960.0\n",
    "11      2385283      2375385.0\n",
    "12      1691973      2385283.0\n",
    "```\n",
    "\n",
    "在上面的例子中，`shift`方法的参数为`1`表示将数据向下移动一个单元，当然我们可以使用参数`-1`将数据向上移动一个单元。相信大家能够想到，如果我们有更多年份的数据，我们可以将参数设置为`12`，这样就可以计算今年的每个月与去年的每个月之间的同比。\n",
    "\n",
    "```python\n",
    "result_df['环比'] = (result_df.本月销售额 - result_df.上月销售额) / result_df.上月销售额\n",
    "result_df.style.format(\n",
    "    formatter={'上月销售额': '{:.0f}', '环比': '{:.2%}'},\n",
    "    na_rep='--------'\n",
    ")\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "      本月销售额      上月销售额         环比\n",
    "月份                    \n",
    "1       5409855       --------     -------- \n",
    "2       4608455        5409855      -14.81%     \n",
    "3       4164972        4608455       -9.62%\n",
    "4       3996770        4164972       -4.04%\n",
    "5       3239005        3996770      -18.96%\n",
    "6       2817936        3239005      -13.00%\n",
    "7       3501304        2817936       24.25%\n",
    "8       2948189        3501304      -15.80%\n",
    "9       2632960        2948189      -10.69%\n",
    "10      2375385        2632960       -9.78%\n",
    "11      2385283        2375385        0.42%\n",
    "12      1691973        2385283      -29.07%\n",
    "```\n",
    "\n",
    "> **说明**：使用 JupyterLab 时，可以通过`DataFrame`对象的`style`属性在网页中对其进行渲染，上面的代码通过`Styler`对象的`format`方法将环比格式化为百分比进行显示，此外还指定了将空值替换为`--------`。\n",
    "\n",
    "更为简单的第二种方案是直接使用`pct_change`方法计算变化的百分比，我们先将之前的上月销售额和环比列删除掉。\n",
    "\n",
    "```python\n",
    "result_df.drop(columns=['上月销售额', '环比'], inplace=True)\n",
    "```\n",
    "\n",
    "接下来，我们使用`DataFrame`对象的`pct_change`方法完成环比的计算。值得一提的是，`pct_change`方法有一个名为`periods`的参数，它的默认值是`1`，计算相邻两项数据变化的百分比，这不就是我们想要的环比吗？如果我们有很多年的数据，在计算时把这个参数的值修改为`12`，就可以得到相邻两年的月同比。\n",
    "\n",
    "```python\n",
    "result_df['环比'] = result_df.pct_change()\n",
    "result_df\n",
    "```\n",
    "\n",
    "### 窗口计算\n",
    "\n",
    "`DataFrame`对象的`rolling`方法允许我们将数据置于窗口中，然后用函数对窗口中的数据进行运算和处理。例如，我们获取了某只股票近期的数据，想制作5日均线和10日均线，那么就需要先设置窗口再进行运算。我们先用如下所示的代码读取2022年百度的股票数据，数据文件可以通过下面的链接来获取。\n",
    "\n",
    "```Python\n",
    "baidu_df = pd.read_excel('data/2022年股票数据.xlsx', sheet_name='BIDU')\n",
    "baidu_df.sort_index(inplace=True)\n",
    "baidu_df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_stock.png\" style=\"zoom:50%;\">\n",
    "\n",
    "上面的`DataFrame`有`Open`、`High`、`Low`、`Close`、`Volume`五个列，分别代表股票的开盘价、最高价、最低价、收盘价和成交量，接下来我们对百度的股票数据进行窗口计算。\n",
    "\n",
    "```Python\n",
    "baidu_df.rolling(5).mean()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_stock_ma5.png\" style=\"zoom:50%;\">\n",
    "\n",
    "我们也可以在`Series`上使用`rolling`设置窗口并在窗口内完成运算，例如我们可以对上面的百度股票收盘价（`Close`列）计算5日均线和10日均线，并使用`merge`函数将其组装到一个`DataFrame`对象中并绘制出双均线图，代码如下所示。\n",
    "\n",
    "```Python\n",
    "close_ma5 = baidu_df.Close.rolling(5).mean()\n",
    "close_ma10 = baidu_df.Close.rolling(10).mean()\n",
    "result_df = pd.merge(close_ma5, close_ma10, left_index=True, right_index=True)\n",
    "result_df.rename(columns={'Close_x': 'MA5', 'Close_y': 'MA10'}, inplace=True)\n",
    "result_df.plot(kind='line', figsize=(10, 6))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_double_MA.png\" style=\"zoom:50%;\">\n",
    "\n",
    "### 相关性判定\n",
    "\n",
    "在统计学中，我们通常使用协方差（covariance）来衡量两个随机变量的联合变化程度。如果变量 $X$ 的较大值主要与另一个变量 $Y$ 的较大值相对应，而两者较小值也相对应，那么两个变量倾向于表现出相似的行为，协方差为正。如果一个变量的较大值主要对应于另一个变量的较小值，则两个变量倾向于表现出相反的行为，协方差为负。简单的说，协方差的正负号显示着两个变量的相关性。方差是协方差的一种特殊情况，即变量与自身的协方差。\n",
    "\n",
    "$$\n",
    "cov(X,Y) = E((X - \\mu)(Y - \\upsilon)) = E(X \\cdot Y) - \\mu\\upsilon\n",
    "$$\n",
    "\n",
    "如果 $X$ 和 $Y$ 是统计独立的，那么二者的协方差为0，这是因为在 $X$ 和 $Y$ 独立的情况下：\n",
    "\n",
    "$$\n",
    "E(X \\cdot Y) = E(X) \\cdot E(Y) = \\mu\\upsilon\n",
    "$$\n",
    "\n",
    "协方差的数值大小取决于变量的大小，通常是不容易解释的，但是正态形式的协方差可以显示两变量线性关系的强弱。在统计学中，皮尔逊积矩相关系数就是正态形式的协方差，它用于度量两个变量 $X$ 和 $Y$ 之间的相关程度（线性相关），其值介于`-1`到`1`之间。\n",
    "\n",
    "$$\n",
    "\\frac {cov(X, Y)} {\\sigma_{X}\\sigma_{Y}}\n",
    "$$\n",
    "\n",
    "估算样本的协方差和标准差，可以得到样本皮尔逊系数，通常用希腊字母 $\\rho$ 表示。\n",
    "\n",
    "$$\n",
    "\\rho = \\frac {\\sum_{i=1}^{n}(X_i - \\bar{X})(Y_i - \\bar{Y})} {\\sqrt{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\sqrt{\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}}\n",
    "$$\n",
    "\n",
    "我们用 $\\rho$ 值判断指标的相关性时遵循以下两个步骤。\n",
    "\n",
    "1. 判断指标间是正相关、负相关，还是不相关。\n",
    "   - 当 $ \\rho \\gt 0 $，认为变量之间是正相关，也就是两者的趋势一致。\n",
    "   - 当 $ \\rho \\lt 0 $，认为变量之间是负相关，也就是两者的趋势相反。\n",
    "   - 当 $ \\rho \\approx 0 $，认为变量之间是不相关的，但并不代表两个指标是统计独立的。\n",
    "2. 判断指标间的相关程度。\n",
    "   - 当 $ \\rho $ 的绝对值在 $ [0.6,1] $ 之间，认为变量之间是强相关的。\n",
    "   - 当 $ \\rho $ 的绝对值在 $ [0.1,0.6) $ 之间，认为变量之间是弱相关的。\n",
    "   - 当 $ \\rho $ 的绝对值在 $ [0,0.1) $ 之间，认为变量之间没有相关性。\n",
    "\n",
    "皮尔逊相关系数适用于：\n",
    "\n",
    "    1. 两个变量之间是线性关系，都是连续数据。\n",
    "    2. 两个变量的总体是正态分布，或接近正态的单峰分布。\n",
    "    3. 两个变量的观测值是成对的，每对观测值之间相互独立。\n",
    "\n",
    "这里，我们顺便说一下，如果两组变量并不是来自于正态总体的连续值，我们该如何判断相关性呢？对于定序尺度（等级），我们可以使用斯皮尔曼秩相关系数，其计算公式如下所示：\n",
    "$$\n",
    "r_{s}=1-{\\frac {6\\sum d_{i}^{2}}{n(n^{2}-1)}}\n",
    "$$\n",
    "其中，$d_{i}=\\operatorname {R} (X_{i})-\\operatorname {R} (Y_{i})$，即每组观测中两个变量的等级差值，$n$为观测样本数。\n",
    "\n",
    "对于定类尺度（类别），我们可以使用卡方检验的方式来判定其是否相关。其实很多时候，连续值也可以通过分箱的方式处理成离散的等级或类别，然后使用斯皮尔曼秩相关系数或卡方检验的方式来判定相关性。\n",
    "\n",
    "`DataFrame`对象的`cov`方法和`corr`方法分别用于计算协方差和相关系数，`corr`方法有一个名为`method`的参数，其默认值是`pearson`，表示计算皮尔逊相关系数；除此之外，还可以指定`kendall`或`spearman`来计算肯德尔系数或斯皮尔曼秩相关系数。\n",
    "\n",
    "我们从名为`boston_house_price.csv`的文件中获取著名的波士顿房价数据集来创建一个`DataFrame`。\n",
    "\n",
    "```python\n",
    "boston_df = pd.read_csv('data/boston_house_price.csv')\n",
    "boston_df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"/Users/Hao/Desktop/Python-Data-Analysis/res/boston_house_price.png\" style=\"zoom:50%;\">\n",
    "\n",
    "> **说明**：上面代码中使用了相对路径来访问 CSV 文件，也就是说 CSV 文件在当前工作路径下名为`data`的文件夹中。如果需要上面例子中的 CSV 文件，可以通过下面的百度云盘地址进行获取。链接：<https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g?pwd=e7b4>，提取码：e7b4。\n",
    "\n",
    "可以看出，该数据集中包含了诸多影响房价的特征，包括犯罪率、一氧化氮浓度、平均房间数、低收入人群占比等，其中`PRICE`代表房价，具体情况如下所示。\n",
    "\n",
    "<img src=\"res/boston_house_price_features.png\" style=\"zoom:50%;\">\n",
    "\n",
    "接下来，我们将其中可以视为来自于正态总体的连续值，通过`corr`方法计算皮尔逊相关系数，看看哪些跟房价是正相关或负相关的关系，代码如下所示。\n",
    "\n",
    "```Python\n",
    "boston_df[['NOX', 'RM', 'PTRATIO', 'LSTAT', 'PRICE']].corr()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/boston_person_correlation.png\" style=\"zoom:50%;\">\n",
    "\n",
    "可以看出，平均房间数（`RM`）跟房价有较强的正相关性，而低收入人群占比（`LSTAT`）跟房价之间存在明显的负相关性。\n",
    "\n",
    "斯皮尔曼秩相关系数对数据条件的要求没有皮尔逊相关系数严格，只要两个变量的观测值是成对的等级数据，或者是由连续变量转化成等级的数据，不论两个变量的总体分布形态、样本容量的大小如何，都可以用斯皮尔曼等级相关系数来进行研究。我们可以通过下面的方式对部分特征进行预处理，然后计算斯皮尔曼秩相关系数。\n",
    "\n",
    "```Python\n",
    "boston_df['CRIM'] = boston_df.CRIM.apply(lambda x: x // 5 if x < 25 else 5).map(int)\n",
    "boston_df['ZN'] = pd.qcut(boston_df.ZN, q=[0, 0.75, 0.8, 0.85, 0.9, 0.95, 1], labels=np.arange(6))\n",
    "boston_df['AGE'] = (boston_df.AGE // 20).map(int)\n",
    "boston_df['DIS'] = (boston_df.DIS // 2.05).map(int)\n",
    "boston_df['B'] = (boston_df.B // 66).map(int)\n",
    "boston_df['PRICE'] = pd.qcut(boston_df.PRICE, q=[0, 0.15, 0.3, 0.5, 0.7, 0.85, 1], labels=np.arange(6))\n",
    "boston_df[['CRIM', 'ZN', 'AGE', 'DIS', 'B', 'PRICE']].corr(method='spearman')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/boston_spearman_correlation.png\" style=\"zoom:50%;\">\n",
    "\n",
    "可以看出，房价跟犯罪率（`CRIM`）和房龄（`AGE`）之间存在较为明显的负相关关系，跟住房用地尺寸（`ZN`）存在微弱的正相关关系。相关性可以帮助我们在实际工作中找到业务抓手，即找到那些能够影响或改变工作结果的相关因素。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283da372-3644-4312-825d-edb165d3deff",
   "metadata": {},
   "source": [
    "### 数据呈现\n",
    "\n",
    "一图胜千言，我们对数据进行透视的结果，最终要通过图表的方式呈现出来，因为图表具有极强的表现力，能够让我们迅速的解读数据中隐藏的价值。和`Series`一样，`DataFrame`对象提供了`plot`方法来支持绘图，底层仍然是通过`matplotlib`库实现图表的渲染。关于`matplotlib`的内容，我们在下一个章节进行详细的探讨，这里我们只简单的讲解`plot`方法的用法。 \n",
    "\n",
    "例如，我们想通过一张柱状图来比较“每个销售区域的销售总额”，可以直接在透视表上使用`plot`方法生成柱状图。我们先导入`matplotlib.pyplot`模块，通过修改绘图的参数使其支持中文显示。\n",
    "\n",
    "```Python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = 'FZJKai-Z03S'\n",
    "```\n",
    "\n",
    "> **说明**：上面的`FZJKai-Z03S`是我电脑上已经安装的一种支持中文的字体的名称，字体的名称可以通过查看用户主目录下`.matplotlib`文件夹下名为`fontlist-v330.json`的文件来获得，而这个文件在执行上面的命令后就会生成。\n",
    "\n",
    "使用魔法指令配置生成矢量图。\n",
    "\n",
    "```Python\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "```\n",
    "\n",
    "绘制“每个销售区域销售总额”的柱状图。\n",
    "\n",
    "```Python\n",
    "temp = pd.pivot_table(df, index='销售区域', values='销售额', aggfunc='sum')\n",
    "temp.plot(figsize=(8, 4), kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "> **说明**：上面的第3行代码会将横轴刻度上的文字旋转到0度，第4行代码会显示图像。\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/sales_bar_graph.png\" style=\"zoom:45%;\">\n",
    "\n",
    "如果要绘制饼图，可以修改`plot`方法的`kind`参数为`pie`，然后使用定制饼图的参数对图表加以定制，代码如下所示。\n",
    "\n",
    "```Python\n",
    "temp.sort_values(by='销售额', ascending=False).plot(\n",
    "    figsize=(6, 6),\n",
    "    kind='pie',\n",
    "    y='销售额',\n",
    "    ylabel='',\n",
    "    autopct='%.2f%%',\n",
    "    pctdistance=0.8,\n",
    "    wedgeprops=dict(linewidth=1, width=0.35),\n",
    "    legend=False\n",
    ")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/sales_pie_graph.png\" style=\"zoom:35%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b024b46-aa54-4a1d-82dd-a0c73167f84c",
   "metadata": {},
   "source": [
    "### index 对象\n",
    "\n",
    "我们再来看看`Index`类型，它为`Series`和`DataFrame`对象提供了索引服务，有了索引我们就可以排序数据（`sort_index`方法）、对齐数据（在运算和合并数据时非常重要）并实现对数据的快速检索（索引运算）。由于`DataFrame`类型表示的是二维数据，所以它的行和列都有索引，分别是`index`和`columns`。`Index`类型的创建的比较简单，通常给出`data`、`dtype`和`name`三个参数即可，分别表示作为索引的数据、索引的数据类型和索引的名称。由于`Index`本身也是一维的数据，索引它的方法和属性跟`Series`非常类似，你可以尝试创建一个`Index`对象，然后尝试一下之前学过的属性和方法在`Index`类型上是否生效。接下来，我们主要看看`Index`的几种子类型。\n",
    "\n",
    "### 范围索引\n",
    "\n",
    "范围索引是由具有单调性的整数构成的索引，我们可以通过`RangeIndex`构造器来创建范围索引，也可以通过`RangeIndex`类的类方法`from_range`来创建范围索引，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "sales_data = np.random.randint(400, 1000, 12)\n",
    "index = pd.RangeIndex(1, 13, name='月份')\n",
    "ser = pd.Series(data=sales_data, index=index)\n",
    "ser\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "月份\n",
    "1     703\n",
    "2     705\n",
    "3     557\n",
    "4     943\n",
    "5     961\n",
    "6     615\n",
    "7     788\n",
    "8     985\n",
    "9     921\n",
    "10    951\n",
    "11    874\n",
    "12    609\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "### 分类索引\n",
    "\n",
    "分类索引是由定类尺度构成的索引。如果我们需要通过索引将数据分组，然后再进行聚合操作，分类索引就可以派上用场。分类索引还有一个名为`reorder_categories`的方法，可以给索引指定一个顺序，分组聚合的结果会按照这个指定的顺序进行呈现，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "sales_data = [6, 6, 7, 6, 8, 6]\n",
    "index = pd.CategoricalIndex(\n",
    "    data=['苹果', '香蕉', '苹果', '苹果', '桃子', '香蕉'],\n",
    "    categories=['苹果', '香蕉', '桃子'],\n",
    "    ordered=True\n",
    ")\n",
    "ser = pd.Series(data=sales_data, index=index)\n",
    "ser\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "苹果    6\n",
    "香蕉    6\n",
    "苹果    7\n",
    "苹果    6\n",
    "桃子    8\n",
    "香蕉    6\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "基于索引分组数据，然后使用`sum`进行求和。\n",
    "\n",
    "```Python\n",
    "ser.groupby(level=0).sum()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "苹果    19\n",
    "香蕉    12\n",
    "桃子     8\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "指定索引的顺序。\n",
    "\n",
    "```python\n",
    "ser.index = index.reorder_categories(['香蕉', '桃子', '苹果'])\n",
    "ser.groupby(level=0).sum()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "香蕉    12\n",
    "桃子     8\n",
    "苹果    19\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "### 多级索引\n",
    "\n",
    "Pandas 中的`MultiIndex`类型用来表示层次或多级索引。可以使用`MultiIndex`类的类方法`from_arrays`、`from_product`、`from_tuples`等来创建多级索引，我们给大家举几个例子。\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "tuples = [(1, 'red'), (1, 'blue'), (2, 'red'), (2, 'blue')]\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=['no', 'color'])\n",
    "index\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "MultiIndex([(1,  'red'),\n",
    "            (1, 'blue'),\n",
    "            (2,  'red'),\n",
    "            (2, 'blue')],\n",
    "           names=['no', 'color'])\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n",
    "index = pd.MultiIndex.from_arrays(arrays, names=['no', 'color'])\n",
    "index\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "MultiIndex([(1,  'red'),\n",
    "            (1, 'blue'),\n",
    "            (2,  'red'),\n",
    "            (2, 'blue')],\n",
    "           names=['no', 'color'])\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "sales_data = np.random.randint(1, 100, 4)\n",
    "ser = pd.Series(data=sales_data, index=index)\n",
    "ser\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "no  color\n",
    "1   red      43\n",
    "    blue     31\n",
    "2   red      55\n",
    "    blue     75\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "ser.groupby('no').sum()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "no\n",
    "1     74\n",
    "2    130\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "ser.groupby(level=1).sum()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "color\n",
    "blue    106\n",
    "red      98\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "stu_ids = np.arange(1001, 1006)\n",
    "semisters = ['期中', '期末']\n",
    "index = pd.MultiIndex.from_product((stu_ids, semisters), names=['学号', '学期'])\n",
    "courses = ['语文', '数学', '英语']\n",
    "scores = np.random.randint(60, 101, (10, 3))\n",
    "df = pd.DataFrame(data=scores, columns=courses, index=index)\n",
    "df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "             语文 数学 英语\n",
    "学号\t学期\t\t\t\n",
    "1001  期中\t93\t77\t60\n",
    "      期末\t93\t98\t84\n",
    "1002  期中\t64\t78\t71\n",
    "      期末\t70\t71\t97\n",
    "1003  期中\t72\t88\t97\n",
    "      期末\t99\t100\t63\n",
    "1004  期中\t80\t71\t61\n",
    "      期末\t91\t62\t72\n",
    "1005  期中\t82\t95\t67\n",
    "      期末\t84\t78\t86\n",
    "```\n",
    "\n",
    "根据第一级索引分组数据，按照期中成绩占`25%`，期末成绩占`75%` 的方式计算每个学生每门课的成绩。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "df.groupby(level=0).agg(lambda x: x.values[0] * 0.25 + x.values[1] * 0.75)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "        语文    数学    英语\n",
    "学号\t\t\t\n",
    "1001\t93.00\t92.75\t78.00\n",
    "1002\t68.50\t72.75\t90.50\n",
    "1003\t92.25\t97.00\t71.50\n",
    "1004\t88.25\t64.25\t69.25\n",
    "1005\t83.50\t82.25\t81.25\n",
    "```\n",
    "\n",
    "### 间隔索引\n",
    "\n",
    "间隔索引顾名思义是使用固定的间隔范围充当索引，我们通常会使用`interval_range`函数来创建间隔索引，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "index = pd.interval_range(start=0, end=5)\n",
    "index\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]], dtype='interval[int64, right]')\n",
    "```\n",
    "\n",
    "`IntervalIndex`有一个名为`contains`的方法，可以检查范围内是否包含了某个元素，如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "index.contains(1.5)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "array([False,  True, False, False, False])\n",
    "```\n",
    "\n",
    "`IntervalIndex`还有一个名为`overlaps`的方法，可以检查一个范围跟其他的范围是否有重叠，如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "index.overlaps(pd.Interval(1.5, 3.5))\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "array([False,  True,  True,  True, False])\n",
    "```\n",
    "\n",
    "如果希望间隔范围是左闭右开的状态，可以在创建间隔索引时通过`closed='left'`来做到；如果希望两边都是关闭状态，可以将`close`参数的值赋值为`both`，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "index = pd.interval_range(start=0, end=5, closed='left')\n",
    "index\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "IntervalIndex([[0, 1), [1, 2), [2, 3), [3, 4), [4, 5)], dtype='interval[int64, left]')\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "index = pd.interval_range(start=pd.Timestamp('2022-01-01'), end=pd.Timestamp('2022-01-04'), closed='both')\n",
    "index\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "IntervalIndex([[2022-01-01, 2022-01-02], [2022-01-02, 2022-01-03], [2022-01-03, 2022-01-04]], dtype='interval[datetime64[ns], both]')\n",
    "```\n",
    "\n",
    "\n",
    "### 日期时间索引\n",
    "\n",
    "`DatetimeIndex`应该是众多索引中最复杂最重要的一种索引，我们通常会使用`date_range()`函数来创建日期时间索引，该函数有几个非常重要的参数`start`、`end`、`periods`、`freq`、`tz`，分别代表起始日期时间、结束日期时间、生成周期、采样频率和时区。我们先来看看如何创建`DatetimeIndex`对象，再来讨论它的相关运算和操作，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "pd.date_range('2021-1-1', '2021-6-30', periods=10)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "DatetimeIndex(['2021-01-01', '2021-01-21', '2021-02-10', '2021-03-02',\n",
    "               '2021-03-22', '2021-04-11', '2021-05-01', '2021-05-21',\n",
    "               '2021-06-10', '2021-06-30'],\n",
    "              dtype='datetime64[ns]', freq=None)\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "pd.date_range('2021-1-1', '2021-6-30', freq='W')\n",
    "```\n",
    "\n",
    "> **说明**：`freq=W`表示采样周期为一周，它会默认星期日是一周的开始；如果你希望星期一表示一周的开始，你可以将其修改为`freq=W-MON`；你也可以试着将该参数的值修改为`12H`，`M`，`Q`等，看看会发生什么，相信你不难猜到它们的含义。\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "DatetimeIndex(['2021-01-03', '2021-01-10', '2021-01-17', '2021-01-24',\n",
    "               '2021-01-31', '2021-02-07', '2021-02-14', '2021-02-21',\n",
    "               '2021-02-28', '2021-03-07', '2021-03-14', '2021-03-21',\n",
    "               '2021-03-28', '2021-04-04', '2021-04-11', '2021-04-18',\n",
    "               '2021-04-25', '2021-05-02', '2021-05-09', '2021-05-16',\n",
    "               '2021-05-23', '2021-05-30', '2021-06-06', '2021-06-13',\n",
    "               '2021-06-20', '2021-06-27'],\n",
    "              dtype='datetime64[ns]', freq='W-SUN')\n",
    "```\n",
    "\n",
    "`DatatimeIndex`可以跟`DateOffset`类型进行运算，这一点很好理解，以为我们可以设置一个时间差让时间向前或向后偏移，具体的操作如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "index = pd.date_range('2021-1-1', '2021-6-30', freq='W')\n",
    "index - pd.DateOffset(days=2)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "DatetimeIndex(['2021-01-01', '2021-01-08', '2021-01-15', '2021-01-22',\n",
    "               '2021-01-29', '2021-02-05', '2021-02-12', '2021-02-19',\n",
    "               '2021-02-26', '2021-03-05', '2021-03-12', '2021-03-19',\n",
    "               '2021-03-26', '2021-04-02', '2021-04-09', '2021-04-16',\n",
    "               '2021-04-23', '2021-04-30', '2021-05-07', '2021-05-14',\n",
    "               '2021-05-21', '2021-05-28', '2021-06-04', '2021-06-11',\n",
    "               '2021-06-18', '2021-06-25'],\n",
    "              dtype='datetime64[ns]', freq=None)\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "index + pd.DateOffset(hours=2, minutes=10)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "```\n",
    "DatetimeIndex(['2021-01-03 02:10:00', '2021-01-10 02:10:00',\n",
    "               '2021-01-17 02:10:00', '2021-01-24 02:10:00',\n",
    "               '2021-01-31 02:10:00', '2021-02-07 02:10:00',\n",
    "               '2021-02-14 02:10:00', '2021-02-21 02:10:00',\n",
    "               '2021-02-28 02:10:00', '2021-03-07 02:10:00',\n",
    "               '2021-03-14 02:10:00', '2021-03-21 02:10:00',\n",
    "               '2021-03-28 02:10:00', '2021-04-04 02:10:00',\n",
    "               '2021-04-11 02:10:00', '2021-04-18 02:10:00',\n",
    "               '2021-04-25 02:10:00', '2021-05-02 02:10:00',\n",
    "               '2021-05-09 02:10:00', '2021-05-16 02:10:00',\n",
    "               '2021-05-23 02:10:00', '2021-05-30 02:10:00',\n",
    "               '2021-06-06 02:10:00', '2021-06-13 02:10:00',\n",
    "               '2021-06-20 02:10:00', '2021-06-27 02:10:00'],\n",
    "              dtype='datetime64[ns]', freq=None)\n",
    "```\n",
    "\n",
    "如果`Series`对象或`DataFrame`对象使用了`DatetimeIndex`类型的索引，此时我们可以通过`asfreq()`方法指定一个时间频率来实现对数据的抽样，我们仍然以之前讲过的百度股票数据为例，给大家做一个演示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "baidu_df = pd.read_excel('data/2022年股票数据.xlsx', sheet_name='BIDU', index_col='Date')\n",
    "baidu_df.sort_index(inplace=True)\n",
    "baidu_df.asfreq('5D')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_stock_asfreq.png\" style=\"zoom:50%;\">\n",
    "\n",
    "大家可能注意到了，每5天抽取1天有可能会抽中非交易日，那么对应的列都变成了空值，为了解决这个问题，在使用`asfreq`方法时可以通过`method`参数来指定一种填充空值的方法，可以将相邻的交易日的数据填入进来。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "baidu_df.asfreq('5D', method='ffill')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_stock_asfreq_ffill.png\" style=\"zoom:50%;\">\n",
    "\n",
    "当使用`DatetimeIndex`索引时，我们也可以通过`resample()`方法基于时间对数据进行重采样，相当于根据时间周期对数据进行了分组操作，分组之后还可以进行聚合统计，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "baidu_df.resample('1M').mean()\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_stock_resample.png\" style=\"zoom:50%;\">\n",
    "\n",
    "代码：\n",
    "\n",
    "```python\n",
    "baidu_df.resample('1M').agg(['mean', 'std'])\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_stock_resample_agg.png\" style=\"zoom:100%;\">\n",
    "\n",
    "> **提示**：不知大家是否注意到，上面输出的`DataFrame` 的列索引是一个`MultiIndex`对象。你可以访问上面的`DataFrame`对象的`columns`属性看看。\n",
    "\n",
    "如果要实现日期时间的时区转换，我们可以先用`tz_localize()`方法将日期时间本地化，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "baidu_df = baidu_df.tz_localize('Asia/Chongqing')\n",
    "baidu_df\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_stock_tz_localize.png\" style=\"zoom:50%;\">\n",
    "\n",
    "在对时间本地化以后，我们再使用`tz_convert()`方法就可以实现转换时区，代码如下所示。\n",
    "\n",
    "代码：\n",
    "\n",
    "```Python\n",
    "baidu_df.tz_convert('America/New_York')\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "<img src=\"res/baidu_stock_tz_convert.png\" style=\"zoom:50%;\">\n",
    "\n",
    "如果你的数据使用了`DatetimeIndex`类型的索引，那么你就很有可能要对数据进行时间序列分析，关于时间序列分析的方法和模型并不是本章节要探讨的内容，我们在其他的专栏中为大家分享。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
